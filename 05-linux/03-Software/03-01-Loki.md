# loki 日志监控集群二进制部署

## 准备

### 准备安装包

> 准备 loki, promtail, grafana 三个包
> loki 是核心, 接收 promtail 专过来的日志并存储, 同时给 grafana 提供查询接口
> promtail 需要在每一台机器上安装, 将机器上的日志发送给 loki
> grafana 从 loki 获取数据进行展示

```
https://github.com/grafana/loki/releases/

# 找到对应版本, 下载以下两个文件:
loki-linux-amd64.zip
promtail-linux-amd64.zip

# 下载 grafana
wget https://dl.grafana.com/enterprise/release/grafana-enterprise-10.2.0.linux-amd64.tar.gz
```

## 安装

### 一. loki 服务端安装

> loki 组件将 promtail 收集上来的日志进行统一存储

#### 1. 解压

```sh
mkdir /usr/local/loki-2.9
unzip loki-linux-amd64.zip -d /usr/local/loki-2.9/
mv /usr/local/loki-2.9/loki-linux-amd64 /usr/local/loki-2.9/loki
```

#### 2. 创建 loki 用户

```sh
useradd loki
```

#### 3. 创建数据目录

```sh
mkdir -p /da1/yun/loki/{config,ruler,compactor,wal,ruler-wal,tsdb-shipper-active,tsdb-shipper-cache,logs}
chown loki:loki -R /da1/yun/loki
```

> PS: 容器部署需要映射目录: `-v /da1/yun/loki:/da1/yun/loki:z`

#### 4. 创建配置文件: `vim /da1/yun/loki/config/config.yaml`

```yaml
# 禁止向 grafana 官方网站发送使用报告
analytics:
  reporting_enabled: false

# target 默认为 all, 可根据功能设置
# write 没有清理表权限, 要想清理表需要添加 table-manager
# target: write,table-manager
# target: read
target: all

# loki 自身不带身份验证功能, 想要认证则必须在前面加一个 nginx 反向代理。使用 nginx 开身份认证. promtail 和 grafana 连接 nginx。
# 这里的 auth_enabled 参数和身份认证没有关系。 仅仅表示是否需要在请求的时候添加 X-Scope-OrgID 请求头。 设置为: true 则请求必须包含 X-Scope-OrgID。 设置为 false 则请求头不必包含 X-Scope-OrgID, loki 会默认设置为: fake
# 也就是说, 即使设置了 auth_enabled: true , 请求中只要带了 X-Scope-OrgID 就还是可以访问, 不需要用户密码
# 建议将  X-Scope-OrgID 的值设置成一个复杂的, 很长的,不容易记的字符串, 当密码使用
auth_enabled: true

# 保留作为镇流器以优化垃圾收集的虚拟内存量（以字节为单位）。 较大的镇流器会减少垃圾收集次数，从而以堆大小为代价减少 CPU 开销。
# 镇流器不会消耗物理内存，因为它永远不会被读取。
# 然而，它会扭曲指标，因为它被算作实时内存。
# 默认: 0, 个人建议别开
ballast_bytes: 0

# 通过 http_listen_port 接收 promtail 发过来的数据; grafana 也通过 http_listen_port 提供日志查询功能
server:
  http_listen_port: 3100
  grpc_listen_port: 9095

# 定义 memberlist, 多个 loki 节点组分布式集群使用
memberlist:
  join_members:
    - 10.57.93.155:7946
    - 10.57.93.156:7946
    - 10.57.93.154:7946
  # 名称
  node_name: ceph-loki
  # 添加随机后缀, 不添加, 则需要 node_name 必须保持区分
  randomize_node_name: true
  # 用于向其他成员通告的IP和端口, 如果是 docker 部署, 需要写宿主机的IP和映射到宿主机的端口, 如果是k8s环境,需要写service名称
  # 将 docker 网络模式改为了 --net=host, 这两个参数没用了
  # advertise_addr: 10.57.93.155
  # advertise_port: 7946
  # 自己监听的用于和其他成员通信端口
  bind_port: 7946


# 得加上这两个目录参数, 要不然会自动在 /loki 目录创建。 以 loki 用户启动没有权限创建 /loki 目录报错
storage_config:
  tsdb_shipper:
    # 摄取者写入索引文件的目录，然后由发货者将其上传到配置的存储
    active_index_directory: /da1/yun/loki/tsdb-shipper-active
    cache_location: /da1/yun/loki/tsdb-shipper-cache

# 配置公共资源, 比如使用 S3 存储等
# 各组件如果在自己的模块中找不到相应配置(比如 ring), 则会最后在 common 这里找
common:
  # 最终只保留了 common 里的 instance_addr 参数, 并且值改为 127.0.0.1 解决了问题, 不过单个查询只能打到一台机器上
  # instance_addr: 10.57.93.155
  # 最终的最终, 将 docker 网络模式改为了 --net=host, 因为设置 instance_addr: 127.0.0.1 之后, 每次查询都只在一个节点上进行。体现不出分布式优势
  # instance_addr: 127.0.0.1
  ring:
    #instance_addr: 10.57.93.155
    #instance_port: 9095
    kvstore:
      store: memberlist
  path_prefix: /loki
  replication_factor: 3
  storage:
    s3:
      endpoint: http://10.57.93.154:8080
      bucketnames: ceph-loki-data
      access_key_id: 8A0BVOEQO4H315ORKWR3
      secret_access_key: lnSWNDfRSKSFxbF6WVKJhP2AFjAmtvsp8nBrml8G
      s3forcepathstyle: true
      insecure: false

# 配置存储类型: tsdb, 配置 index 和 chunks
schema_config:
  configs:
    - from: 2023-11-22
      object_store: s3
      schema: v12
      store: tsdb
      index:
        prefix: loki_index_
        period: 24h
      chunks:
        prefix: loki_chunk_
        period: 24h

# 用于处理基于日志数据的告警规则
# 它负责根据用户定义的规则，从Loki中的日志数据中筛选出匹配的日志事件，并生成相应的告警
ruler:
  wal:
    dir: /da1/yun/loki/ruler-wal
  rule_path: /da1/yun/loki/ruler
  storage:
    type: s3
    s3:
      bucketnames: ceph-loki-ruler

# 存储日志时 chunk 相关设置
ingester:
  wal:
    enabled: true
    dir: /da1/yun/loki/wal
  chunk_block_size: 262144
  chunk_target_size: 1572864
  max_chunk_age: 2h
  chunk_idle_period: 30m

# 压缩相关
compactor:
  # 指定compactor的工作目录，用于存储临时文件和其他工作相关的数据
  working_directory: /da1/yun/loki/compactor
  # 指定compactor使用的共享存储后端。在这种情况下，它是"S3"，表示使用Amazon S3作为存储后端。
  shared_store: s3
  # 指定compactor执行一次数据压缩的时间间隔。表示每30分钟执行一次数据压缩。
  compaction_interval: 30m
  # 指定是否启用数据自动删除功能, 开启则会删除过期的数据
  retention_enabled: true
  # 指定从索引表中删除过期数据的延迟时间, 即过期的日志数据将在2小时后被删除
  retention_delete_delay: 2h
  # 指定用于删除过期数据的工作线程数量
  retention_delete_worker_count: 150

# 资源限制相关(部分参数)
limits_config:
  # 是否允许时间乱序写入, 默认允许(true), 一般不用改
  unordered_writes: true
  # 默认 global 会将 ingestion_burst_size_mb 的值除以 distributor 数量
  # 修改为 local
  ingestion_rate_strategy: local
  # 每个用户每秒的采样率限制，默认4MB
  ingestion_rate_mb: 64
  # 每个用户允许的采样突发大小，默认6MB
  ingestion_burst_size_mb: 128
  per_stream_rate_limit: 128MB
  per_stream_rate_limit_burst: 256MB
  # 查询返回的最大日志条目
  max_entries_limit_per_query: 20000
  # 数据保留 31 天, 将 31 天前日志全部删掉
  retention_period: 744h
  # 每个用户可以创建的最大流的数量
  max_streams_per_user: 0
  # 全局可以创建的流的最大数量
  max_global_streams_per_user: 600000
  # 拒绝 超过 reject_old_samples_max_age 之前的样本数据
  reject_old_samples: true
  # 拒绝 7 天前的样本数据
  reject_old_samples_max_age: 168h
  # 查询的最大范围, 查询时间范围超过该值将被拒绝
  max_query_length: 744h
  # 查询的最大并行度。指定同时执行查询的最大数量
  max_query_parallelism: 256
  # 索引表的基数限制。基数是指索引表中唯一标签组合的数量。如果索引表的基数超过此限制，Loki将停止为该标签组合创建新的索引表
  cardinality_limit: 100000

# 查询调度
query_scheduler:
  max_outstanding_requests_per_tenant: 32768

# 查询
querier:
  max_concurrent: 16

query_range:
  # 是否缓存数据
  cache_results: true
  # 使用嵌入式缓存, 缓存 16G 日志数据, 缓存不压缩
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 200
  # 是否缓存索引, 如果为 true 但没设置 index_stats_results_cache 参数, 则使用 results_cache 作为缓存
  cache_index_stats_results: true
  # 使用嵌入式缓存, 缓存 16G 日志索引, 缓存不压缩
  index_stats_results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 200

# 缓存设置, 等查询有人喊慢的时候再加
# chunk_store_config:
#   #日志写入存储前的缓存配置
#   chunk_cache_config:
#   # 删除重复写入的日志缓存配置
#   write_dedupe_cache_config:
#   # 缓存早于该时间段的索引条目
#   cache_lookups_older_than:

# 使用 ceph 的 rgw 存储数据, 不支持使用 table_manager, 应该使用对象存储的 custom bucket policy 来删除老数据
# table_manager 在使用本地存储时才有效
table_manager:
  retention_deletes_enabled: true
  retention_period: 720h
```

#### 5. 修改配置文件所属用户

```sh
chown loki:loki /da1/yun/loki/config/config.yaml
```

#### 6. 创建 service 启动文件

> `vim /usr/lib/systemd/system/loki.service`

```toml
[Unit]
Description=Loki service
After=network.target

[Service]
Type=simple
User=loki
ExecStart=/usr/local/loki-2.9/loki -config.file /da1/yun/loki/config/config.yaml
Restart=on-failure
RestartSec=20

[Install]
WantedBy=multi-user.target
```

#### 7. 设置开启自启动, 并启动实例

```sh
systemctl enable loki.service
systemctl start loki.service
```

#### 8. 查看集群是否正常

> 在浏览器打开 `http://10.57.93.154:3100/ring`

```
浏览器打开 http://10.57.93.154:3100/ring 后, 看到表格中有三行记录, 表示3个节点已经组成集群。
状态(State) 列显示三个节点都是 ACTIVE 表示集群正常 
```

### 二. 安装 nginx 反向代理

> loki 自身不带身份验证功能, 如果想加认证则必须在前面加一个 nginx 反向代理。 nginx 开身份认证.
> loki 的auth_enabled 设置为 true, 与认证没有关系, 只表示是否需要租户ID请求头

#### 1. 安装 nginx

```sh
docker pull ecr-sh.yun.lsne.cn/alkaid/nginx@sha256:1bb5c4b86cb7c1e9f0209611dc2135d8a2c1c3a6436163970c99193787d067ea
```

#### 2. 创建密钥文件

```
echo -n 'loki.ceph.yun:' > /etc/nginx/conf.d/.htpasswd
openssl passwd -apr1 123456 >> /etc/nginx/conf.d/.htpasswd

# 为了安全,最好在设置一下权限
chown nginx:nginx /etc/nginx/conf.d/.htpasswd
chmod 600 /etc/nginx/conf.d/.htpasswd
```

#### 3. 编辑配置文件: `vim /etc/nginx/nginx.conf`

```nginx
user nginx;
# 工作线程数, 默认1
worker_processes 8;
error_log /var/log/nginx/error.log;
pid /run/nginx.pid;

include /usr/share/nginx/modules/*.conf;

events {
    worker_connections 65535;
}

http {
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile            on;
    tcp_nopush          on;
    tcp_nodelay         on;
    keepalive_timeout   65;
    types_hash_max_size 4096;

    include             /etc/nginx/mime.types;
    default_type        application/octet-stream;

    include /etc/nginx/conf.d/*.conf;

    # 指定用于解析主机名的DNS服务器
    # resolver 127.0.0.1;

    # 定义两组后端服务器(目前暂时两组的后端都一样)
    upstream reads {
    server     10.57.93.155:3100;
    server     10.57.93.156:3100;
    server     10.57.93.154:3100;
    }
    upstream writes {
    server     10.57.93.155:3100;
    server     10.57.93.156:3100;
    server     10.57.93.154:3100;
    }

    # 存储客户端请求头的缓冲区大小
    large_client_header_buffers 4 32k;

    server {
        # nginx 监听端口, 认证名称, 认证用户密码文件, 和访问 loki 的请求头, 认证名称,认证密码文件,请求头也可以写到下级的 location 内部
        listen             3200;
        auth_basic "loki authentication";
        auth_basic_user_file /etc/nginx/conf.d/.htpasswd;

        # loki 要求如果开启身份认证, 则必须设置 X-Scope-OrgID 请求头。 也可以在 grafana 配置数据源时设置请求头
        # proxy_set_header X-Scope-OrgID "ceph";

        location = / {
            return 200 'OK';

            # 取消继承自上级的身份认证体系
            auth_basic off;
        }

        location = /api/prom/push {
            proxy_pass       http://writes;
        }

        location = /api/prom/tail {
            proxy_pass       http://reads;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }

        location ~ /api/prom/.* {
            proxy_pass       http://reads;
        }

        location = /loki/api/v1/push {
            proxy_pass       http://writes;
        }

        location = /loki/api/v1/tail {
            proxy_pass       http://reads;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }

        location ~ /loki/api/.* {
            proxy_pass       http://reads;
        }
    }
}
```

#### 4. 启动 nginx 

```sh
docker run -d -p 3200:3200 -v /etc/nginx:/etc/nginx ecr-sh.yun.lsne.cn/alkaid/nginx@sha256:1bb5c4b86cb7c1e9f0209611dc2135d8a2c1c3a6436163970c99193787d067ea
```

#### 5. grafana 配置

> 用户名 和 X-Scope-OrgID 不必相同

```
http://10.57.93.155:3200

Basic Authentication
loki.ceph.yun
123456

X-Scope-OrgID: loki.ceph.yun
```


### 三. 安装 promtail 日志收集 agent

> promtail 组件将本机机器上传给 loki 服务端
> 每台机器都需要安装一个 promtail

#### 1. 解压

```sh
mkdir /usr/local/promtail-2.9/
unzip promtail-linux-amd64.zip -d /usr/local/promtail-2.9/
mv /usr/local/promtail-2.9/promtail-linux-amd64 /usr/local/promtail-2.9/promtail
```

#### 2. 创建 promtail 用户

```sh
useradd promtail
```

#### 3. 创建数据目录

```sh
mkdir -p /da1/yun/promtail/{config,logs}
chown promtail:promtail -R /da1/yun/promtail
```

#### 4. 创建配置文件: `vim /da1/yun/promtail/config/config.yaml`

```yaml
server:
  # 禁用 HTTP 和 GRPC 服务
  disable: false
  http_listen_port: 9080
  # 0 表示随机端口
  grpc_listen_port: 0
  log_level: "info"
clients:
  - url: http://10.57.93.155:3100/loki/api/v1/push

    # key: value 形式的自定义 http 请求头信息
    # headers:

    # 默认使用的租户 ID，用于推送日志到 Loki。
    # 如果省略或为空，则会假设 Loki 在单租户模式下运行，不发送 X-Scope-OrgID 头。
    # 建议搞一个复杂的, 超级长的, 不容易记住的字符串, 当密码使用
    tenant_id: loki.ceph.yun

    # 发送一批日志前的最大等待时间，即使该批次日志数据未满。
    batchwait: 1s

    # 在向 Loki 发送批处理之前要积累的最大批处理量（以字节为单位）。
    batchsize: 102400

    # 如果使用了 basic auth 认证，则需要配置用户名和密码
    basic_auth:
      username: "loki.ceph.yun"
      password: "123456"
    #   # 包含basic auth认证的密码文件
    #   password_file: 

    # 发送给服务器的 Bearer token
    # bearer_token: 

    # # 包含 Bearer token 的文件
    # bearer_token_file: 

    # # 用来连接服务器的 HTTP 代理服务器
    # proxy_url: 

    # # 如果连接到一个 TLS 服务器，配置 TLS 认证方式。
    # tls_config:
    #   # 用来验证服务器的 CA 文件
    #   ca_file: 
    #   # 发送给服务器用于客户端认证的 cert 文件
    #   cert_file: 
    #   # 发送给服务器用于客户端认证的密钥文件
    #   key_file: 
    #   # 验证服务器证书中的服务器名称是这个值。
    #   server_name: 
    #   # 如果为 true，则忽略由未知 CA 签署的服务器证书。默认: false
    #   insecure_skip_verify: true

    # 配置在请求失败时如何重试对 Loki 的请求。
    # 默认的回退周期为：
    # 0.5s, 1s, 2s, 4s, 8s, 16s, 32s, 64s, 128s, 256s(4.267m)
    # 在日志丢失之前的总时间为511.5s(8.5m)
    backoff_config:
      # 重试之间的初始回退时间
      min_period: 500ms
      # 重试之间的最大回退时间
      max_period: 5m
      # 重试的最大次数
      max_retries: 10

    # 如果后面有用到 pipeline_stages.metrics 定义给 prometheus 获取的监控指标, 则指标中不会出现这里定义的标签。还是得在 scrape_configs.static_configs 定义静态标签才行
    # 添加到所有发送到 Loki 的日志中的静态标签
    # 使用一个类似于 {"foo": "bar"} 的映射来添加一个 foo 标签，值为 bar
    # 这些也可以从命令行中指定：
    # -client.external-labels=k1=v1,k2=v2
    # (或 --client.external-labels 依赖操作系统)
    # 由命令行提供的标签将应用于所有在 "clients" 部分的配置。
    # 注意：如果标签的键相同，配置文件中定义的值将取代命令行中为特定 client 定义的值
    # external_labels:
    #   application: ceph
    #   cluster: lsne-test
    #   host: oss04.cpp.zzbm.lsne.cn
    # 等待服务器响应一个请求的最长时间
    timeout: 10s

positions:
  # positions 文件的路径
  filename: /da1/yun/promtail/config/positions.yaml

  # 更新 positions 文件的周期
  sync_period: 10s

  # 是否忽略并覆盖被破坏的 positions 文件
  ignore_invalid_yaml: true

limits_config:
  # 强制执行速率限制, 默认: false
  readline_rate_enabled: false
  # 可以推送到 Loki 的每秒日志行数, 默认: 10000
  readline_rate: 100000
  # 可以推送的突发行数上限, 默认: 10000
  readline_burst: 100000
  # true: 超过限制直接丢弃删除日志, false: 暂时推迟发送日志行并稍后重试。 默认: true
  readline_rate_drop: false
  # 限制流数量, 可以有效避免占用内存太多导致的OOM, 默认: 0 不限制
  max_streams: 100
  # 日志行太长的话, 是否截断。设置为 false 则 max_line_size 参数不生效。 默认: false
  max_line_size_truncate: false
  # 允许每行的最大日志大小,  Example: 256kb, 2M. 默认: 0 不限制
  max_line_size: 2M

scrape_configs:
- job_name: ceph
  # 解压行为
  decompression:
    # 是否尝试解压, 默认: false
    enabled: false
  static_configs:
  - targets:
    - localhost
    labels:
      application: ceph
      cluster: lsne-test
      host: oss04.cpp.zzbm.lsne.cn
      service: rgw
      __path__: /da1/var/log/ceph/ceph*rgw*.log
  - labels:
      application: ceph
      cluster: lsne-test
      host: oss04.cpp.zzbm.lsne.cn
      service: rgw
      __path__: /da1/var/log/ceph/*/ceph*rgw*.log
  - labels:
      application: ceph
      cluster: lsne-test
      host: oss04.cpp.zzbm.lsne.cn
      service: rgw
      __path__: /da1/ceph/log/rgw/ceph*rgw*.log
  - labels:
      application: ceph
      cluster: lsne-test
      host: oss04.cpp.zzbm.lsne.cn
      service: rgw
      __path__: /var/log/ceph/ceph*rgw*.log
  - labels:
      application: ceph
      cluster: lsne-test
      host: oss04.cpp.zzbm.lsne.cn
      service: rgw
      __path__: /var/log/ceph/*/ceph*rgw*.log
  # 是一个数组, 按数组顺序操作: 先用 regex 或 json 对 static_configs 定义的文件按行提取数据, 然后将提取的行数据按之后的数组内容进行操作
  pipeline_stages:
  - drop:
      expression: '.*======.starting.*'
  - regex:
      expression: ".*ERROR:(?P<reason>.*)"
  - labeldrop:
      - filename
  - metrics:
      http_errer_total:
        type: Counter
        description: "rgw error total count"
        prefix: ceph_rgw_
        source: reason
        config:
          action: inc
  - regex:
      expression:  .*======.*latency=(?P<latency>\S+)
  - metrics:
      http_latency:
        type: Histogram
        description: "rgw return http response latency"
        source: latency
        prefix: ceph_rgw_
        config:
          bucket: [0.00001,0.0001,0.001,0.1,1,5]
  - regex:
      expression: (?P<date>^\S+\s\S+)(\s\S*){5}\S\s(?P<client>(\d*.){5})\s-\s-\s(?P<request_date>\S+\s\S+)\s"(?P<method>\w+)\s(?P<url>/(?P<bucket>\w+(-|\w)*)\S+)\s(?P<http_version>HTTP/\d.\d)"\s(?P<res_code>\d+)\s(?P<body_size>\d+)
  - labels:
      res_code:
      bucket:
      method:
  - metrics:
      http_code_total:
        type: Counter
        description: "rgw return http response code total"
        source: res_code
        prefix: ceph_rgw_
        config:
          action: inc
      http_out_width:
        type: Counter
        description: "rgw return http response bytes total"
        source: body_size
        prefix: ceph_rgw_
        config:
          action: add
  - labeldrop:
      - "method"
      - "res_code"
  - metrics:
      bukcet_info:
        type: Gauge
        description: "only get bucket name"
        prefix: ceph_rgw_
        source: bucket
        config:
          action: inc
  - labeldrop:
      - "bucket"
```

#### 4.2 根据业务需求, promtail 配置文件中 pipeline_stages 部分还可以添加以下内容

#### 4.2.1 pipeline_stages

> pipeline_stages 是一个数组, 按数组顺序对行记录进行操作
> 比如: 
> 数组第一个元素的指令: 筛选出包含 error 的行
> 数组第二个元素的指令: 将 error 行进行统计, 每增加一行 error 记录就加1
> 数组第三个元素的指令: 将记录中指定字段提取出来做为 loki 标签使用

```
  # 是一个数组, 按数组顺序操作: 先用 regex 或 json 对 static_configs 定义的文件按行提取数据, 然后将提取的行数据按之后的数组内容进行操作
  pipeline_stages:
  # 脱色, 将具有颜色代码的每一行转换为非颜色代码
  - decolorize:
  # 只允许哪些标签传到 loki 服务端, 只限制动态标签, 不会限制 static_configs.labels 和 clients.external_labels 指定的标签
  - labelallow:
    - application
    - cluster
    - host
    - service
  # 删除动态标签
  - labeldrop:
      - service
  # 从之前的步骤提取的 map 中添加标签
  - labels:
      stream:
  # 将指标进行统计, 但不会发送到 loki, 而是需要 promtheus 通过 127.0.0.1:9080/metrics/ 接口进行获取
  - metrics:
  # 将日志采样后发送给 loki， 比如只发送 十分之一 的日志到 loki
  - sampling:
  # 将多行合并为一行, 可以指定第一行的正则表达式, 但是 ceph N 版本的日志太乱了。没办法合并
  - multiline:
      pattern: '^\d{4}-\d{2}-\d{2}'  # 正则表达式用于匹配日志行的起始
      match: after                  # 合并模式，可以是"before"或"after"
      negate: false                 # 是否合并不匹配的行，默认为false
      max_lines: 10                 # 最大合并行数，默认为10
      timeout: 5s                   # 合并超时时间，默认为5秒
```

#### 5. 修改配置文件所属用户

```sh
chown promtail:promtail -R /da1/yun/promtail/config/config.yaml
```

#### 6. 创建 service 启动文件

> `vim /usr/lib/systemd/system/promtail.service`

```toml
[Unit]
Description=Loki service
After=network.target

[Service]
Type=simple
User=promtail
ExecStart=/usr/local/promtail-2.9/promtail -config.file /da1/yun/promtail/config/config.yaml
Restart=on-failure
RestartSec=20

[Install]
WantedBy=multi-user.target
```

#### 7. 设置开启自启动, 并启动实例

```
systemctl enable promtail.service
systemctl start promtail.service
```


## 附录1: 命令行参数

```sh
./loki \
-config.file=/usr/local/loki/loki-local-config.yaml \
-print-config-stderr \  # 命令行启动时直接将配置输出到屏幕上, 好像没啥用
-log-config-reverse-order \ # 可以在 grafana 界面查看配置
```

## 附录2: chunk 相关

```
在Loki中，这些参数用于管理日志数据的分块和存储。下面是这些参数的含义：

chunk_block_size（分块块大小）：这是指每个日志块（chunk）的大小。日志数据会被分成多个块进行存储，这个参数定义了每个块的大小。较大的块大小可以提高查询性能，但会增加内存和存储的消耗。

chunk_target_size（分块目标大小）：这是指Loki在写入日志数据时希望每个块达到的目标大小。当写入的日志数据超过目标大小时，Loki会触发块的切割操作，将数据拆分为更小的块。目标大小的选择需要根据具体的使用情况进行权衡，以平衡存储和查询性能。

max_chunk_age（最大块年龄）：这是指每个块的最大存储时间。当一个块的存储时间达到最大块年龄时，Loki会将其标记为不可写入，并在后续查询中忽略该块。这个参数可以用来控制数据的保留期限，以及在查询时排除过时的数据。

chunk_idle_period（块空闲期）：这是指块在不被写入或查询时的空闲时间。当一个块在一段时间内没有被写入或查询时，Loki会将其标记为可压缩状态，并在后续的压缩操作中将其压缩为更小的块。这个参数可以用来控制块的压缩频率，以减少存储空间的使用。

这些参数可以根据实际需求进行调整，以平衡存储空间的使用和查询性能。
```

## 附录3: MongoDB 日志的配置文件

```yaml
server:
  disable: false
  http_listen_port: 9080
  grpc_listen_port: 0
  log_level: "info"
clients:
  - url: http://10.57.93.155:3100/loki/api/v1/push
    tenant_id: loki.ceph.yun
    batchwait: 1s
    batchsize: 102400
    basic_auth:
      username: "loki.ceph.yun"
      password: "123456"
    backoff_config:
      min_period: 500ms
      max_period: 5m
      max_retries: 10
    timeout: 10s
positions:
  filename: /da1/yun/promtail/config/positions.yaml
  sync_period: 10s
  ignore_invalid_yaml: true
limits_config:
  readline_rate_enabled: false
  readline_rate: 100000
  readline_burst: 100000
  readline_rate_drop: false
  max_streams: 100
  max_line_size_truncate: false
  max_line_size: 2M
scrape_configs:
- job_name: mongodb
  decompression:
    enabled: false
  static_configs:
  - targets:
    - localhost
    labels:
      application: mongodb
      host: oss04.cpp.zzbm.lsne.cn
      service: mongos
      __path__: /data1/mongodb35*/log/mongod.log
   - labels:
      application: mongodb
      host: oss04.cpp.zzbm.lsne.cn
      service: config
      __path__: /data1/mongodb25*/log/mongod.log
   - labels:
      application: mongodb
      host: oss04.cpp.zzbm.lsne.cn
      service: shard
      __path__: /data1/mongodb4[789]*/log/mongod.log
  - labels:
      application: mongodb
      host: oss04.cpp.zzbm.lsne.cn
      service: mongod
      __path__: /data1/mongodb45*/log/mongod.log
```