# 故障排查

### 日志查看

#### 查看运行时参数配置

```sh
ceph daemon {daemon-name} config show | less

# 示例
ceph daemon osd.0 config show | less
```

#### 查看集群日志报警的详细信息

```sh
ceph health detail
```
#### 运行时激活 Ceph 的调试日志输出

```sh
ceph tell {daemon-type}.{daemon id or *} config set {name} {value}

# 示例
ceph tell osd.0 config set debug_osd 0/5

# 本地使用 socket 方式修改运行时参数
sudo ceph daemon osd.0 config set debug_osd 0/5
```

#### 配置文件方式开启调试日志输出示例

```toml
[global]
    debug_ms = 1/5

[mon]
    debug_mon = 20
    debug_paxos = 1/5
    debug_auth = 2

 [osd]
     debug_osd = 1/5
     debug_filestore = 1/5
     debug_journal = 1
     debug_monc = 5/20

[mds]
    debug_mds = 1
    debug_mds_balancer = 1
```


### ceph rgw 容量大小不一致问题排查

>  radosgw-admin bucket stats --bucket=xxx 显示的实际大小, 与实际文件大小不一致
>  bucket stats 显示  320G大小,  12万个文件
>  实际只有  20G大小,  9万个文件

#### 查看所有 12万个文件的元数据

```
radosgw-admin bi list --bucket scanlog > scanlog.info

# 发现有9万个正常文件
# 有 3 万个异常文件: 异常文件以: _multipart 开头, 表示大文件切片上传的部分切片
# 统计 3万个 _multipart 文件大小: 300G 与实际的9万个文件20G相加后为 bucket stats 中显示的 320G大小。 存储大小不一致的原因找到了
```

### osd 启动失败

#### journalctl 日志报错

```
fastbmap_allocator_impl.h: In function 'void AllocatorLevel02<T>::_mark_allocated(uint64_t, uint64_t) [with L1 = AllocatorLevel01Loose; uint64_t = long unsigned int]' thread 2b5ab5c38bc0 time 2024-01-25 17:00:40.255837
fastbmap_allocator_impl.h: 750: FAILED ceph_assert(available >= allocated)
```

#### 解决办法

```toml
[osd]
bluefs_alloc_size = 65536
```

### osd 启动报错

#### journalctl 日志报错

```
Dec 06 19:29:19 oss21.cpp.zll.lsne.cn ceph-osd-run.sh[1690]: -140> 2023-12-06 19:29:18.360 7f18037f2dc0 -1 bdev(0x558bba320e00 /var/lib/ceph/osd/ceph-228/block.db) _aio_start io_setup(2) failed with EAGAIN; try increasing /proc/sys/fs/aio-max-nr
Dec 06 19:29:19 oss21.cpp.zll.lsne.cn ceph-osd-run.sh[1690]: -139> 2023-12-06 19:29:18.361 7f18037f2dc0 -1 bluestore(/var/lib/ceph/osd/ceph-228) _minimal_open_bluefs add block device(/var/lib/ceph/osd/ceph-228/block.db) returned:(11) Resource temporarily unavailable
Dec 06 19:29:19 oss21.cpp.zll.lsne.cn ceph-osd-run.sh[1690]: -5> 2023-12-06 19:29:18.972 7f18037f2dc0 -1 bdev(0x558bba321180 /var/lib/ceph/osd/ceph-228/block) _aio_start io_setup(2) failed with EAGAIN; try increasing /proc/sys/fs/aio-max-nr
Dec 06 19:29:19 oss21.cpp.zll.lsne.cn ceph-osd-run.sh[1690]: -4> 2023-12-06 19:29:18.972 7f18037f2dc0 -1 bluestore(/var/lib/ceph/osd/ceph-228) _minimal_open_bluefs add block device(/var/lib/ceph/osd/ceph-228/block) returned: (11)Resource temporarily unavailable
```

#### 查看 aio-max-nr 值

```sh
[root@oss21 lsne]# cat /proc/sys/fs/aio-max-nr
65536
```

#### 修改 aio-max-nr 值

```sh
vim /etc/sysctl.conf
fs.aio-max-nr = 131072
```

#### 使修改生效

```sh
sysctl -p
```

#### 重启 osd

```sh
systemctl restart ceph-osd@252.service
```

### rgw 经常自动重启，内核崩溃

> 怀疑是 linux 内核比较低的原因

#### 报错日志

```sh
Dec 06 02:51:39 oss20.cpp.zll.lsne.cn docker[88129]: *** Caught signal (Segmentation fault) **
Dec 06 02:51:39 oss20.cpp.zll.lsne.cn docker[88129]: in thread 7fef7dbb3700 thread_name:civetweb-worker
Dec 06 02:51:39 oss20.cpp.zll.lsne.cn docker[88129]: teardown: managing teardown after SIGCHLD
Dec 06 02:51:39 oss20.cpp.zll.lsne.cn docker[88129]: teardown: Waiting PID 100 to terminate
Dec 06 02:51:39 oss20.cpp.zll.lsne.cn docker[88129]: teardown: Process 100 is terminated
Dec 06 02:51:39 oss20.cpp.zll.lsne.cn docker[88129]: teardown: Bye Bye, container will die with return code 0
Dec 06 02:51:40 oss20.cpp.zll.lsne.cn docker[61642]: Error response from daemon: No such container: ceph-rgw-oss20-rgw0
Dec 06 02:51:50 oss20.cpp.zll.lsne.cn systemd[1]: ceph-radosgw@rgw.oss20.rgw0.service holdoff time over, scheduling restart.
Dec 06 02:51:50 oss20.cpp.zll.lsne.cn systemd[1]: Stopped Ceph RGW.
```

#### 查看当前 kernel 版本

```sh
[root@oss20 lsne]# uname -a
Linux oss20.cpp.zll.lsne.cn 3.10.0-693.21.1.el7.p0.x86_64 #1 SMP Sat May 16 11:22:15 CST 2020 x86_64 x86_64 x86_64 GNU/Linux
```

#### 查看 repo 可安装的内核版本

```sh
[root@oss20 lsne]# yum list | grep kernel
kernel.x86_64                            3.10.0-693.21.1.el7.p0        @/kernel-3.10.0-693.21.1.el7.p0.x86_64
kernel.x86_64                            3.10.0-1160.el7               @anaconda
```

#### 升级内核

```sh
yum update kernel -y
```

#### 重启机器

> 如果三台 rgw 同时也是三台 mon 节点, 则需要一台一台重启。一台重启完成，并加入集群后, 再重启另外一台

```
reboot
```


### 磁盘 IO 显示耗时高

>  现象: 磁盘 IO 显示耗时高， 会频繁出现1秒以上的请求

```
grafana 监控图
```

#### 办法一: 修改 linux 内核启动参数

> 添加内核启动参数: `scsi_mod.use_blk_mq=1, dm_mod.use_blk_mq=y`, 修改调度器

```sh
vim /etc/default/grub
...
GRUB_CMDLINE_LINUX="audit=0 rd.driver.pre=ixgbe,igb crashkernel=auto spectre_v2=retpoline rd.lvm.lv=VolGroup00/LogVol00 nodmraid selinux=0 biosdevname=1 net.ifnames=1 scsi_mod.use_blk_mq=1 dm_mod.use_blk_mq=y rhgb quiet console=tty0"
...

grub2-mkconfig -o /boot/grub2/grub.cfg
# reboot  # 需要重启机器生效
```

#### 办法二: 修改 `nr_requests` 参数

> 默认值为: 128, 要修改为: 32  最后使用的是上面修改调度器的方法, 这个参数改为32未实践

```sh
for i in `ls -d /sys/block/sd*`; do echo 32 > $i/queue/nr_requests ;done

# 后续需要修改 /etc/default/grub 永久生效
```

### Q版本查看下线的osd对应的db分区

#### 查看 ceph-volume 中分区使用情况

```sh
# 输出结果为 block 未使用的分区
ceph-volume lvm list   | grep "db device" | sort | uniq -u
```

#### 查看逻辑卷并与 ceph-volume 做对比

```sh
lvdisplay | grep "/dev/ceph-" | grep osd-db

#对比
ceph-volume lvm list   | grep "db device" | sort | uniq
```

#### 格式化磁盘

```sh
# 这里不加 --destroy 参数, 因为 /dev/ceph-xxx/osd-db-xxx 这个逻辑卷需要保留
ceph-volume lvm zap /dev/sdg /dev/ceph-a108db22-733d-4475-ab12-ef95fc26cacb/osd-db-530ee57e-acfd-4a0c-87cd-800efc09d699

# 加 --destroy 参数会销毁逻辑卷
ceph-volume lvm zap --destroy /dev/sdg
```