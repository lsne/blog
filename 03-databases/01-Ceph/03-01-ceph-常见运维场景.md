# ceph 常见运维场景

### 在 prometheus 查看 rgw 耗时

> 该监控项是通过 `promtail` 将日志采集到 `loki` 的同时, 又保留了每行日志的 http 请求分析结果提供给 `promtheus` 获取得到的,  需要在 `promtheus` 上执行以下查询语句查询

```sh
histogram_quantile(0.90, sum(rate(ceph_rgw_http_latency_bucket{cluster="oss-zzbm"}[5m])) by (le,cluster,instance))
```

### 参数修改

> 修改 backfills 均衡速度相关参数

```sh
for i in `ceph osd df |awk '{print $1}' |grep -Ev 'TOTAL|MIN|ID'`;do ceph tell osd.$i injectargs --osd_max_backfills=2 --osd_recovery_max_active=15 --osd_recovery_max_single_start=5 --osd_delete_sleep=900;done
```

### 修改集群 osd 参数

#### 修改集群参数

```sh
# 设置 osd down, 或者权重设置为0, 或者 out 出去，不会自动均衡数据
ceph osd set nobackfill

# 设置 osd out (即使是手动执行的  ceph osd out x 命令), 集群也不会实际执行 out 动作
ceph osd set noout

# 取消设置
ceph osd unset nobackfill
ceph osd unset noout
```

#### 参数在实例中修改

```sh
for i in `ceph osd df |awk '{print $1}' |grep -Ev 'TOTAL|MIN|ID'`;do ceph tell osd.$i injectargs --osd_max_backfills=50 --osd_recovery_max_active=200 --osd_recovery_max_single_start=50 --osd_delete_sleep=900 --osd_map_cache_size=50 --mon_osd_down_out_interval=900;done
```

### cephadm 集群修改配置文件

#### 找到 active 状态的 mgr 节点

```sh
ceph -s
```

#### 登录 active 状态的 mgr 节点, 并修改配置文件

> `vim /var/lib/ceph/d6bb9a12-bf69-11eb-bfd8-fa163d8c7748/mgr.centos701v.jl.bjzt.lsne.cn.jmgsrw/config`

```toml
# 修改, 添加, 或删除想调整的参数
[osd]
osd_max_backfills=50
osd_recovery_max_active=200
osd_recovery_max_single_start=50
osd_delete_sleep=900
osd_map_cache_size=50
mon_osd_down_out_interval=900
```

#### 重启 active 状态的 mgr 节点

```sh
docker restart 6f9c393be6fe
```

#### 查找 daemon name

```sh
ceph orch ls | awk '{print $1}'
ceph orch ps | grep mgr |awk '{print $1}'
```

#### 根据daemon name，依次更新daemon

```sh
ceph orch daemon reconfig mgr.xxx # 更新 非active 状态的 mgr config
ceph orch reconfig mon # 更新 mon service config
ceph orch reconfig osd # 更新 osd service config
ceph orch reconfig crash # 更新 crash service config
...... # 通过ceph orch ls 查出的其他service
```

#### 使用配置生效: 方法一

> 直接重启对应服务

```sh
ceph orch restart rgw
```

#### 使用配置生效: 方法二

```
使用 ceph tell 方式对实例进行修改
```

### ceph-deploy 集群修改配置文件

#### 进入部署机器的 ceph 集群部署目录

```sh
ssh storages301.dlc.zzbm.lsne.cn
cd /ceph/ceph-cluster
```

#### 修改当前部署目录下的 `vim ceph.conf` 文件

```toml
# 修改, 添加, 或删除想调整的参数
[osd]
osd_max_backfills=50
osd_recovery_max_active=200
osd_recovery_max_single_start=50
osd_delete_sleep=900
osd_map_cache_size=50
mon_osd_down_out_interval=900
```

#### 将配置推送到指定的节点上

```sh
ceph-deploy --username=lsne --overwrite-conf config push myk8smaster04v.cpp.zzt.lsne.cn myk8smaster05v.cpp.zzt.lsne.cn
```

#### 配置文件生效: 方法一

> 直接重启对应服务

```sh
ssh myk8smaster05v.cpp.zzt.lsne.cn
systemctl restart ceph-osd@1152.service
```

#### 配置文件生效: 方法二

```
使用 ceph tell 方式对实例进行修改
```

### ceph-deploy 添加 rgw 

> 以 `yangben-s3-zzbm` 集群为例

```
# 示例中添加两台 rgw 
osses11.yanben.zzbm.lsne.cn
osses12.yanben.zzbm.lsne.cn
```

#### 部署管理机器和目录

```sh
ssh storages301.dlc.zzbm.lsne.cn
cd /ceph/ceph-cluster
```

#### 编辑配置文件: `vim /ceph/ceph-cluster/ceph.conf`

> 添加以下 rgw 配置, 要添加几台 rgw 节点, 则配置文件要添加几个 [client.rgw.]

```toml
[client.rgw.osses11.yanben.zzbm.lsne.cn]
host = osses11.yanben.zzbm.lsne.cn
keyring = /var/lib/ceph/radosgw/ceph-rgw.osses11.yanben.zzbm.lsne.cn/keyring
log file = /da1/ceph/log/rgw/ceph-client.rgw.osses11.yanben.zzbm.lsne.cn.log
rgw_frontends = civetweb port=7480+443s num_threads=20000 ssl_certificate=/cert/_.b.lsne.cn_chained.crt
rgw_s3_auth_use_rados = true
rgw_s3_auth_use_keystone = false
rgw_s3_auth_aws4_force_boto2_compat = false
rgw_s3_auth_use_ldap = false
rgw_s3_success_create_obj_status = 0
rgw_relaxed_s3_bucket_names = false
rgw_s3_auth_use_sts =true
rgw_sts_key = abcdefghijklmnop
#debug_rgw = 20
rgw_cache_expiry_interval = 3600
rgw_cache_lru_size = 200000
rgw op thread suicide timeout = 30
rgw_enable_gc_threads=true
rgw_gc_max_concurrent_io=10
rgw_gc_max_objs=1000
rgw_gc_max_trim_chunk=16
rgw_gc_obj_min_wait=120
rgw_gc_processor_max_time=3600
rgw_gc_processor_period=3600
rgw_nfs_max_gc=300
rgw_objexp_gc_interval=600

[client.rgw.osses12.yanben.zzbm.lsne.cn]
host = osses12.yanben.zzbm.lsne.cn
keyring = /var/lib/ceph/radosgw/ceph-rgw.osses12.yanben.zzbm.lsne.cn/keyring
log file = /da1/ceph/log/rgw/ceph-client.rgw.osses12.yanben.zzbm.lsne.cn.log
rgw_frontends = civetweb port=7480+443s num_threads=20000 ssl_certificate=/cert/_.b.lsne.cn_chained.crt
rgw_s3_auth_use_rados = true
rgw_s3_auth_use_keystone = false
rgw_s3_auth_aws4_force_boto2_compat = false
rgw_s3_auth_use_ldap = false
rgw_s3_success_create_obj_status = 0
rgw_relaxed_s3_bucket_names = false
rgw_s3_auth_use_sts =true
rgw_sts_key = abcdefghijklmnop
#debug_rgw = 20
rgw_cache_expiry_interval = 3600
rgw_cache_lru_size = 200000
rgw op thread suicide timeout = 30
rgw_enable_gc_threads=true
rgw_gc_max_concurrent_io=10
rgw_gc_max_objs=1000
rgw_gc_max_trim_chunk=16
rgw_gc_obj_min_wait=120
rgw_gc_processor_max_time=3600
rgw_gc_processor_period=3600
rgw_nfs_max_gc=300
rgw_objexp_gc_interval=600

```

#### 将 cert 文件复制到要安装 rgw 的机器

```sh
ssh osses11.yanben.zzbm.lsne.cn
ssh osses12.yanben.zzbm.lsne.cn
分别登录两台机器执行:
mkdir /cert
vim /cert/_.b.lsne.cn_chained.crt
将 集群其他 rgw 机器上的相同文件内容复制过来
```

#### 对每一个新机器生成权限密钥文件

```sh
ceph-authtool --create-keyring /etc/ceph/ceph.client.rgw.osses11.yanben.zzbm.lsne.cn.keyring
chmod +r /etc/ceph/ceph.client.rgw.osses11.yanben.zzbm.lsne.cn.keyring
ceph-authtool /etc/ceph/ceph.client.rgw.osses11.yanben.zzbm.lsne.cn.keyring -n client.rgw.osses11.yanben.zzbm.lsne.cn --gen-key
ceph-authtool -n client.rgw."$i" --cap osd 'allow rwx' --cap mon 'allow rw' /etc/ceph/ceph.client.rgw.osses11.yanben.zzbm.lsne.cn.keyring
ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw."$i" -i /etc/ceph/ceph.client.rgw.osses11.yanben.zzbm.lsne.cn.keyring

ceph-authtool --create-keyring /etc/ceph/ceph.client.rgw.osses12.yanben.zzbm.lsne.cn.keyring
chmod +r /etc/ceph/ceph.client.rgw.osses12.yanben.zzbm.lsne.cn.keyring
ceph-authtool /etc/ceph/ceph.client.rgw.osses12.yanben.zzbm.lsne.cn.keyring -n client.rgw.osses12.yanben.zzbm.lsne.cn --gen-key
ceph-authtool -n client.rgw."$i" --cap osd 'allow rwx' --cap mon 'allow rw' /etc/ceph/ceph.client.rgw.osses12.yanben.zzbm.lsne.cn.keyring
ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw."$i" -i /etc/ceph/ceph.client.rgw.osses12.yanben.zzbm.lsne.cn.keyring
```

#### 将生成的密钥文件复制到对应的机器上

> 复制到对应机器上的 `/var/lib/ceph/radosgw/ceph-rgw.$(hostname)/keyring` 目录

```sh
scp /etc/ceph/ceph.client.rgw.osses11.yanben.zzbm.lsne.cn.keyring lsne@osses11.yanben.zzbm.lsne.cn:/home/lsne/
scp /etc/ceph/ceph.client.rgw.osses12.yanben.zzbm.lsne.cn.keyring lsne@osses12.yanben.zzbm.lsne.cn:/home/lsne/

然后登录 osses11, osses12 执行
cp ceph.client.rgw.*.keyring /var/lib/ceph/radosgw/ceph-rgw.`hostname`/keyring
```

#### 创建 rgw 

```sh
ssh storages301.dlc.zzbm.lsne.cn
cd /ceph/ceph-cluster
ceph-deploy --username=lsne --overwrite-conf rgw create osses11.yanben.zzbm.lsne.cn
ceph-deploy --username=lsne --overwrite-conf rgw create osses12.yanben.zzbm.lsne.cn
```

#### 查看新机器上 7480 和 443 端口是否正常

```sh
netstat -alntp | grep radosgw | grep -E ":7480|:443"
```

#### 后续处理

```
后续可能会涉及到将新的 RGW 实例IP添加到 VIP 的 RS解析上. 或者需要修改域名等操作
```

### s3 跨域访问

#### 创建跨域配置文件: `vim cors.xml`

> 将以下内容中 `https://paoding.b.qiainxin-inc.cn` 替换为实际要修改的集群地址, 然后保存文件

```xml
<CORSConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
    <CORSRule>
        <AllowedMethod>GET</AllowedMethod>
        <AllowedMethod>PUT</AllowedMethod>
        <AllowedMethod>DELETE</AllowedMethod>
        <AllowedMethod>HEAD</AllowedMethod>
        <AllowedMethod>POST</AllowedMethod>
        <AllowedOrigin>https://s3-zzbm.b.lsne.com</AllowedOrigin>
        <AllowedHeader>*</AllowedHeader>
        <MaxAgeSeconds>3600</MaxAgeSeconds>
        <ExposeHeader>*</ExposeHeader>
    </CORSRule>
</CORSConfiguration>
```

#### 执行以下命令将配置文件应用到指定的 bucket 中

> 以 `mybucket01` 为例

```sh
s3cmd setcors cors.xml s3://mybucket01
```

#### 批量执行

```sh
for i in $(s3cmd ls | grep "s3://scan_log" | awk '{print $3}'); do s3cmd setcors cors.xml $i;done
```

### s3 设置策略禁止写操作

#### 创建策略文件: `vim scan_log_policy.json`

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ScanLogUserDenyAllWriteObject",
      "Effect": "Deny",
      "Principal": {
        "AWS": ["arn:aws:iam:::user/scan_log"]
    },
      "Action": [
        "s3:PutObject",
        "s3:PutObjectAcl",
        "s3:PutObjectVersionAcl",
        "s3:DeleteObject",
        "s3:DeleteObjectVersion"
      ],
      "Resource": [
        "arn:aws:s3:::scan_log*/*"
      ]
    }
  ]
}
```

#### 对 bucket 设置策略

```sh
s3cmd setpolicy scan_log_policy.json s3://scan_log0
```

#### 批量执行

```sh
for i in $(s3cmd ls | grep "s3://scan_log" | awk '{print $3}'); do s3cmd setpolicy scan_log_policy.json $i;done
```

### 设置 sts 角色

#### 开启 rgw 的 sts 支持

 > 编辑 ceph 配置文件 `vim /etc/ceph/ceph.conf`

```toml
# 添加以下两个参数, 开启 sts 支持

[client.rgw.mytestceph03v.rgw0]

# 用于加密会话令牌的sts密钥, 官方说必须是 16 个字符组成的十六进制字符串。 我这里设置这么长好像也没问题
rgw_sts_key = OISHJDOI2093089HDSFG290lHsdfgSIUOElknR2SCF

# 开启 sts 支持
rgw_s3_auth_use_sts = true
```

> 重启所有 rgw

```sh
systemctl restart ceph-radosgw@rgw.mytestceph03v.rgw0.service
```

#### 创建角色

> 编辑策略文件

```json
[root@mytestceph01v lsne]# cat user_policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam:::user/myuser00aa"
        ]
      },
      "Action": [
        "sts:AssumeRole"
      ]
    }
  ]
}
```

> 创建角色

```sh
radosgw-admin role create --role-name=myuser01-assume --path="/" --assume-role-policy-doc=\{\"Version\":\"2012-10-17\",\"Statement\":\[\{\"Effect\":\"Allow\",\"Principal\":\{\"AWS\":\[\"arn:aws:iam:::user/myuser01\"\]\},\"Action\":\[\"sts:AssumeRole\"\]\}\]\}
```

#### 测试

```sh
radosgw-admin role-policy add --role-name qsafelog-acl-test --policy-name policy1 --role-policy-doc "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"s3:PutObject\"],\"Resource\":[\"arn:aws:s3:::qsafelog/images/*.jpg\", \"arn:aws:s3:::qsafelog/images/*.jpeg\", \"arn:aws:s3:::qsafelog/files/*.exe\"]}]}"

 radosgw-admin role-policy put --role-name qsafelog-acl-test --policy-name policy1 --role-policy-doc "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"s3:PutObject\"],\"Resource\":[\"arn:aws:s3:::qsafelog/images/*.jpg\", \"arn:aws:s3:::qsafelog/images/*.jpeg\", \"arn:aws:s3:::qsafelog/files/*.exe\"]}]}"

 radosgw-admin role-policy put --role-name qsafelog-acl-test --policy-name policy1 --policy-doc 
 
radosgw-admin role-policy put --role-name qsafelog-acl-test --policy-name policy1 --policy-doc "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":[\"s3:PutObject\"],\"Resource\":[\"arn:aws:s3:::qsafelog/log/*.log\", \"arn:aws:s3:::qsafelog/log/*.commit\", \"arn:aws:s3:::qsafelog/log/*.log.gz\", \"arn:aws:s3:::qsafelog/log/*.zip\", \"arn:aws:s3:::qsafelog/images/*.xbm\", \"arn:aws:s3:::qsafelog/images/*.jpg\", \"arn:aws:s3:::qsafelog/images/*.ico\", \"arn:aws:s3:::qsafelog/images/*.png\", \"arn:aws:s3:::qsafelog/images/*.pjpeg\", \"arn:aws:s3:::qsafelog/images/*.tif\", \"arn:aws:s3:::qsafelog/images/*.tiff\", \"arn:aws:s3:::qsafelog/images/*.webp\", \"arn:aws:s3:::qsafelog/images/*.bmp\", \"arn:aws:s3:::qsafelog/images/*.avif\"]}]}"
```

### 设置 rgw 压缩模式

#### 查看当前压缩模式

```sh
radosgw-admin zone placement get --placement-id default-placement
```

#### 设置 snappy 压缩

```sh
radosgw-admin zone placement modify --rgw-zone default --placement-id default-placement --storage-class STANDARD --compression snappy
```

### 调整磁盘权重

#### 查看磁盘使用率

```sh
ceph osd df |sort -n -k 18
```

#### 调整权重

```sh
ceph osd crush reweight osd.432 9
```

### 磁盘下线

#### osd 权重调整为 0

```sh
ceph osd crush reweight osd.393 0
```

#### 等待数据均衡完成

```sh
ceph -s
```

#### 调整 osd 为 out 状态

> 或者(14.2.4-0 以及之后版本)可以不用 out 操作, 直接在后面执行 `purge` 命令
> 
```sh
ceph osd out 393
```

#### 登录 osd 所在机器, 关闭进程

```sh
systemctl disable ceph-osd@393
systemctl stop ceph-osd@393
```

#### 集群中删除 osd

```sh
ceph osd purge 393 --yes-i-really-mean-it

# 或
ceph osd purge osd.393 --yes-i-really-mean-it
```

#### ceph 集群中将机器从 carsh rule map 里删除

```sh
ceph osd crush rm sceph22
```

#### 或者(旧版)

```sh
ceph osd crush remove osd.xxxx
ceph osd rm osd.xxxx
ceph auth del osd.xxxx
```

### 修改 s3cmd 上传文件类型

> 不使用 `file` 命令的方式，而是使用文件扩展名后缀方式

```sh
s3cmd --no-use-mime-magic
```

### ceph 开启监控

#### 查看 mgr 的参数配置

```sh
# 在 N 版本不可用, N 版只能查看具体的每个实例的参数配置。
ceph config get mgr mgr/prometheus/stale_cache_strategy
ceph config get mgr mgr/prometheus/scrape_interval

# 查看所有mgr实例的配置
for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do ceph config get mgr.$i mgr/prometheus/scrape_interval; ceph config get mgr.$i mgr/prometheus/stale_cache_strategy; done;

# 查看所有配置(这个操作 N 版也可以用)
ceph config dump
```

#### 删除每个实例单独设置的参数(如果有设置)

```sh
# 删除每个实例设置的参数
for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do echo "ceph config rm mgr.$i mgr/prometheus/scrape_interval; ceph config rm mgr.$i mgr/prometheus/stale_cache_strategy"; done;

for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do ceph config rm mgr.$i mgr/prometheus/scrape_interval; ceph config rm mgr.$i mgr/prometheus/stale_cache_strategy; done;
```

#### 设置

```sh
ceph config set mgr mgr/prometheus/stale_cache_strategy return

ceph config set mgr mgr/prometheus/scrape_interval 60

# 设置统计 rbd 状态信息, 如果是 Q 版本可以使用 * 统计所有池子
ceph config set mgr mgr/prometheus/rbd_stats_pools cinder.volumes.hdd,cinder.volumes.ssd,cinder.volumes.ssd-02,cinder.volumes.hdd-02

# 定期扫描刷新指定池和命名空间下的所有文件列表
# 如果模块检测到来自先前未知的 RBD 图像的统计信息，该模块将强制提前刷新。
ceph config set mgr mgr/prometheus/rbd_stats_pools_refresh_interval 300
```

#### 重启 prometheus 模块

```sh
ceph mgr module disable prometheus

ceph mgr module enable prometheus
```
