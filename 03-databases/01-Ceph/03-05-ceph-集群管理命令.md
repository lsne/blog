# ceph 运维常用命令

> 标题以后再改, 根据操作内容进行分类整理

## 问题排查处理

### 一. 告警信息排查

#### 1. 查看集群是否有告警

> `ceph -s`  的 `health:` 部分

```sh
root@mytestceph11v:~# ceph -s
  cluster:
    id:     41e02e26-02f3-4eab-97b4-3e01a7e03c85
    health: HEALTH_ERR
            mons are allowing insecure global_id reclaim
            Module 'devicehealth' has failed: disk I/O error
            1 mgr modules have recently crashed
```

#### 2. 查看告警详情

> `ceph health detail`

```sh
root@mytestceph11v:~# ceph health detail
HEALTH_ERR mons are allowing insecure global_id reclaim; Module 'devicehealth' has failed: disk I/O error; 1 mgr modules have recently crashed
[WRN] AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED: mons are allowing insecure global_id reclaim
    mon.mytestceph11v has auth_allow_insecure_global_id_reclaim set to true
    mon.mytestceph12v has auth_allow_insecure_global_id_reclaim set to true
    mon.mytestceph13v has auth_allow_insecure_global_id_reclaim set to true
[ERR] MGR_MODULE_ERROR: Module 'devicehealth' has failed: disk I/O error
    Module 'devicehealth' has failed: disk I/O error
[WRN] RECENT_MGR_MODULE_CRASH: 1 mgr modules have recently crashed
    mgr module devicehealth crashed in daemon mgr.mytestceph11v on host mytestceph11v.lsn.cn at 2024-02-23T00:09:06.344060Z
```

#### 3. 告警: `insecure global_id reclaim`

> 表示允许 “不安全回收”的客户端连接集群

##### 3.1 查看会话信息

> `ceph tell mon.\* sessions`

```
# 用于向所有 Monitor 进程发送一个名为 `sessions` 的消息。这个命令的作用是用来查看当前 Monitor 进程的会话信息，包括哪些客户端正在连接到 Monitor 进程，以及它们的会话状态等。
```

##### 3.2 暂时消除此告警

> `ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1w   # 1 week`

##### 3.3 永久消除此告警

> `ceph config set mon mon_warn_on_insecure_global_id_reclaim false`

##### 3.2 禁止 “不安全回收”的客户端连接集群

> `ceph config set mon auth_allow_insecure_global_id_reclaim false`

#### 4. 消除告警 && 取消消除告警

```sh
ceph health mute <code>
ceph health unmute <code>

# 示例
ceph health mute OSD_DOWN
ceph health unmute OSD_DOWN
```
#### 4. 监控日志

> `ceph -w`

#### 5. 查看最近的日志

> `ceph log last [n]`


## MGR 命令

### 一. 常用命令

#### 1. 强制节点故障转移

> 在集群任意节点执行, 会将当前active的 mgr 节点变为从节点, 重新选一个其他节点做为active节点

```sh
ceph mgr fail
```

## PG 命令

### 一. 常用命令

#### 1. 查看 pg 状态

```sh
ceph pg dump
```

#### 2. 查看指定 pg 所在的 OSD

```sh
ceph pg map {pg-num}
```

#### 3. 查看 pg 状态

```sh
ceph pg stat
```

#### 4. 查询特定的 PG

```sh
ceph pg {poolnum}.{pg-id} query
```

#### 5. 查看不正常的PG

```sh
ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]
```

#### 6. 查找对象

```sh
ceph osd map {poolname} {object-name} [namespace]

示例
ceph osd map data test-object-1
osdmap e537 pool 'data' (1) object 'test-object-1' -> pg 1.d1743484 (1.4) -> up ([0,1], p0) acting ([0,1], p0)
```

## PG 问题处理

### 一. PG 问题诊断

#### 1. 查看 ceph 集群运行状态详细信息

```sh
ceph health detail
```

#### 2. 查看 PG 状态详细信息

```sh
ceph pg dump --format=json-pretty
```

#### 3. 查看不一致的PG列表

```sh
rados list-inconsistent-pg {pool}
```

#### 4. 查看 PG 中不一致的对象列表

```sh
rados list-inconsistent-obj {pgid}
```

#### 5. 查看特定 PG **中不一致快照集的列表

```sh
rados list-inconsistent-snapset {pgid}
```

### 二. 修复 PG

#### 1. 修复损坏的 PG

```sh
ceph pg repair {pgid}
```

## Pool 命令

#### 1.1 给 Pool 创建快照

```sh
# 给池创建快照
ceph osd pool mksnap {pool-name} {snap-name}
```

#### 1.2 删除 pool 快照

```sh
ceph osd pool rmsnap {pool-name} {snap-name}
```

#### 2. 查看所有 pool 列表

```sh
# 只显示 pool 名
ceph osd pool ls

# 显示 pool id 和 pool 名
ceph osd lspools
```

#### 3. 查看所有 pool 的详细信息

```sh
ceph osd pool ls detail
```


#### 4. 创建 pool

```sh
ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] [replicated] [crush-rule-name] [expected-num-objects]

# 或
ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] erasure [erasure-code-profile] [crush-rule-name] [expected_num_objects] [--autoscale-mode=<on,off,warn>]
```

#### 5. 设置 pool 应用类型

```sh
# 查看应用类型
ceph osd pool application get lstest

# 设置应用类型
# [cephfs,rbd,rgw] 三种类型可选
ceph osd pool application enable lstest rbd

# 或直接初始化为 rbd 类型
rbd pool init lstest
```

#### 6. 设置 pool 最大字节或最大对象数量

```sh
ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}]

# 示例设置 lstest 池子最大对象数为 10000, (设置为0表示取消配额限制)
ceph osd pool set-quota lstest max_objects 10000
```

#### 7. 删除 pool 

```sh
# 必须启用 mon_allow_pool_delete 参数, 并且 pool 上也必须允许删除
ceph osd pool rm pool001 pool001 --yes-i-really-really-mean-it
```

#### 8. 删除没有池子使用的 crush_rule

```sh
# 先查看池子使用的规则
ceph osd pool get {pool-name} crush_rule

# 示例
ceph osd pool get myfsdata crush_rule

# 批量查看指定的规则(如规则ID: 12 )是否还被池子使用
ceph osd dump | grep "^pool" | grep "crush_rule 12"
```

#### 9. 删除只有某池权限的用户

```sh
# 查看只有 myfsdata 池子权限的用户
ceph auth ls | grep -C 5 myfsdata

# 删除用户
ceph auth del {user}
```

#### 10. 重命名池

> 重命名后,  只授权了该池权限的用户需要重新授权

```sh
ceph osd pool rename {current-pool-name} {new-pool-name}
```

#### 11. 查看池的大小,对象数等信息

```sh
rados df
```

#### 12. 查看池的IO信息

```sh
# 查看所有池
ceph osd pool stats

# 查看指定的池: default.rgw.buckets.data
ceph osd pool stats default.rgw.buckets.data
```

#### 13. 获取 pool 参数

```sh
ceph osd pool get {pool-name} {key}
```
#### 14. 设置 pool 参数

```sh
ceph osd pool set {pool-name} {key} {value}
```

#### 15. 设置 最小副本数量

> 任何对象在副本数少于 min_size 时都不会接收 I/O

```sh
ceph osd pool set data min_size 2
```

#### 16. 设置池大小

```sh
ceph osd pool set mypool target_size_bytes 100T

ceph osd pool set mypool target_size_ratio 1.0

# target_size_ratio 1.0 解释
# 如果只有这一个池, 则使用 100%
# 如果一共两个池, 每个池都是 target_size_ratio 1.0, 则每个池都可以使用 50%
```

## MDS 命令

#### 1. 查看状态

```sh
ceph fs status
```

#### 2. 查看 mds 的主机名

```sh
ceph mds metadata ssdfs_gpt.ceph05.dgquwl
```

#### 3. 查看连接

```sh
# 管理节点
ceph tell mds.* session ls

# mds 节点
ceph daemon /var/run/ceph/b2ef654a-0905-11ee-88f3-946dae908c22/ceph-mds.ssdfs_gpt.ceph05.dgquwl.asok session ls
```

#### 4. 清理指定的连接

```sh
ceph daemon mds.<MDS Daemon ID> session evict <session ID>
```

#### 5. 清理所有连接

```sh
ceph daemon mds.<MDS Daemon ID> session ls

```

## 纠删码

```sh
# 纠删码不能使用: Filestore 存储, 只能使用: BlueStore
# 纠删码只建议在 rgw pool 中使用

# ceph L版本以后, 纠删码可以在 RBD 和 CephFS 中使用, 但需要设置:
ceph osd pool set ec_pool allow_ec_overwrites true

# 纠删码不支持omap, 所以使用 RBD 和 CephFS 时, 需要手动指定:
1. 元数据池使用副本模式    -- 元数据不支持纠删码
2. 数据存储池使用 EC 模式

rbd 使用纠删码
rbd create --size 1G --data-pool ec_pool replicated_pool/image_name

CephFS 可以在文件系统创建期间或通过文件布局将纠删码池设置为默认数据池
```

#### 1. 创建纠删码 pool 

> 最简单的纠删码池类似于 RAID5，并且需要至少三台主机

```sh
ceph osd pool create mypool001 erasure
```

#### 2. 查看默认配置文件

> 默认配置文件: 允许两个 OSD 重叠丢失而不丢失数据
> 安全性相当 3 副本的复制池
> 存储 1TB数据: 3 副本模式需要 3TB 存储, 默认纠删码配置文件: 需要 2TB 存储

```sh
ceph osd erasure-code-profile get default

k=2
m=2
plugin=jerasure
technique=reed_sol_van
```

#### 3. 创建配置文件

```sh
# 创建 EC 配置文件 myprofile
ceph osd erasure-code-profile set myprofile k=3 m=2 crush-failure-domain=rack

# k=3 存储的数据对象将被分为3个部分
# m=2 同时丢失多少个OSD而不丢失任何数据
# crush-failure-domain=rack 确保不会有两个块存储在一个 rack 中


# 将创建好的 myprofile 配置文件应用到池 ecpool001
ceph osd pool create ecpool001 erasure myprofile
```

## 归置组

#### 1. 自动缩放归置组

```sh
# 全局设置
ceph config set global osd_pool_default_pg_autoscale_mode <mode>

# 指定所有池
ceph osd pool get noautoscale
ceph osd pool set noautoscale
ceph osd pool unset noautoscale

# 指定池设置
ceph osd pool set <pool-name> pg_autoscale_mode <mode>

# mode:
# off: 禁用此池的自动缩放
# on: 启用给定池的 PG 计数的自动调整
# warn：当 PG 数量需要调整时，提高健康检查

# 示例
ceph osd pool set foo pg_autoscale_mode on
```

#### 2. 查看 PG 缩放建议

```sh
# N版需要开启 pg_autoscaler 模块, 否则执行命令会报错
ceph osd pool autoscale-status
```

#### 3. 每个OSD目标PG数量

```sh
# 自动缩放有用, 默认: 100
ceph config set global mon_target_pg_per_osd 100
```

#### 4. 池设置 bulk 标志

```sh
# 只对自动均衡有用

# 创建一个有 bulk 标志的池
ceph osd pool create <pool-name> --bulk

# 查看池的 bulk 标志
ceph osd pool get <pool-name> bulk

# 设置或取消 bulk 标志
ceph osd pool set <pool-name> bulk <true/false/1/0>
```

#### 5. 设置池的最大最小值

```sh
# 创建池时
ceph osd pool create --pg-num-min <num> --pg-num-max <num>

# 设置指定的池
ceph osd pool set <pool-name> pg_num_min <num>
ceph osd pool set <pool-name> pg_num_max <num>
```

#### 6. 调整 pg 数量

```sh
ceph osd pool get {pool-name} pg_num
ceph osd pool set {pool-name} pg_num {pg_num}
ceph osd pool set {pool-name} pgp_num {pgp_num}
```

#### 7. 查看 pg 状态

```sh
ceph pg dump [--format {format}]
```

#### 8. 查看不正常的pg状态

```sh
ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format <format>] [-t|--threshold <seconds>]
```

#### 9. 获取 pg map

```sh
ceph pg map {pg-id}

# 示例
ceph pg map 1.6c

# 示例输出
osdmap e13 pg 1.6c (1.6c) -> up [1,0] acting [1,0]
```

#### 10. 获取 pg 统计状态

```sh
ceph pg {pg-id} query
```

#### 11. scrub 指定的 PG

```sh
ceph pg scrub {pg-id}
```

#### 12. scrub 指定 pool 中的所有 pg

```sh
ceph osd pool scrub {pool-name}
```

#### 13. 设置指定的PG优先进行 recovery 和 backfill

> 不会中断当前的 backfill 和 recovery ，只是将指定的pg放到队列顶部

```sh
ceph pg force-recovery {pg-id} [{pg-id #2}] [{pg-id #3} ...]
```

#### 14. 取消PG优先 recovery 和 backfill 的设置

```sh
ceph pg cancel-force-recovery {pg-id} [{pg-id #2}] [{pg-id #3} ...]
ceph pg cancel-force-backfill {pg-id} [{pg-id #2}] [{pg-id #3} ...]
```

#### 15. 设置指定的 pool 中的所有 PG 优先进行 recovery 和 backfill

```sh
ceph osd pool force-recovery {pool-name}
ceph osd pool force-backfill {pool-name}
```

#### 16. 取消指定的 pool 中的 PG优先 recovery 和 backfill 的设置

```sh
ceph osd pool cancel-force-recovery {pool-name}
ceph osd pool cancel-force-backfill {pool-name}
```

#### 17. 设置 pool 在 recovery 和 backfill 时的优先级

> 数值越大,  优先级越高, 设置了值比默认值优先级高

```sh
ceph osd pool set {pool-name} recovery_priority {value}
```

#### 18. 标记丢失对象

> 标记恢复到之前版本, 或直接标记为删除

```sh
ceph pg {pg-id} mark_unfound_lost revert|delete
```

## PG-UPMAP

> OSDMap中有一个pg-upmap异常表，允许集群显式地将特定PG映射到特定OSD。 这使得集群可以微调数据分布，以便在大多数情况下在 OSD 之间均匀分布 PG。
> 警告：它要求所有客户端了解 OSDMap 中新的 pg-upmap 结构。
> 使用 pg-upmap 时, 集群不能有任何 L 版之前的客户端

#### 1. 关闭均衡器

```sh
ceph balancer off
```

#### 2. 设置所有客户端不低于L版本

> pg-upmap 功能要求所有客户端不能低于 L 版, 如果有L版之前的版本的客户端, 则执行此命令会失败

```sh
ceph osd set-require-min-compat-client luminous
```

#### 3. 查看正在使用的客户端版本

```sh
ceph features
```

### 二. 离线优化

#### 1. 获取 osdmap 的最新副本

```sh
ceph osd getmap -o om
```

#### 2. 运行优化器

```sh
osdmaptool om --upmap out.txt [--upmap-pool <pool>] \
[--upmap-max <max-optimizations>] \
[--upmap-deviation <max-deviation>] \
[--upmap-active]
```

## 主节点是否均衡

> read_balance_score  数值高于1, 表示不均衡

## balancer

> 如果 PG 发生降级, 则 balancer 不会对 PG 分配进行任何调整

> upmap 模式, crush-compat 模式

#### 1. 查看状态

```sh
ceph balancer status
```

#### 2. 禁用 balancer

```sh
ceph balancer off
```

#### 3. 调整最大错位比例

```sh
# 默认 0.05 即 5%
# 设置为 0.07 即 7%
ceph config set mgr target_max_misplaced_ratio .07
```

#### 4. 均衡器运行间隔(秒)

```sh
ceph config set mgr mgr/balancer/sleep_interval 60
```

#### 5. 均衡器开启时间

```sh
# HHMM 格式
# 00:00 开始
ceph config set mgr mgr/balancer/begin_time 0000
```

#### 6. 均衡器结束时间

```sh
# HHMM 格式
# 23:59 结束
ceph config set mgr mgr/balancer/end_time 2359
```

#### 7. 均衡器每周几运行

```sh
# 设置每周日开启均衡
ceph config set mgr mgr/balancer/begin_weekday 0
```

#### 8. 均衡器每周几结束

```sh
ceph config set mgr mgr/balancer/end_weekday 6
```

#### 9. 指定 pool 自动均衡

> 默认空字符串表示所有 pool 都开启自动均衡

```sh
ceph config set mgr mgr/balancer/pool_ids 1,2,3
```

#### 10. 修改均衡模式

> L 版本之前默认模式为: crush-compat
> L 版本之后默认模式为: upmap

```sh
ceph balancer mode crush-compat
```

#### 11. 监督优化

```sh
分三个阶段
1. 指定计划
2. 评估数据分布的质量，无论是当前的 PG 分布还是执行计划后产生的 PG 分布
3. 执行计划
```

##### 11.1 评估当前的分布
`
```sh
# 正常评估
ceph balancer eval

# 详细评估
ceph balancer eval-verbose
```

##### 11.2 评估单个 pool 的分布

```sh
ceph balancer eval <pool-name>
```

##### 11.3 生成计划

```sh
ceph balancer optimize <plan-name>
```

##### 11.4 查看生成的计划内容

```sh
ceph balancer show <plan-name>
```

##### 11.5 显示所有计划

```sh
ceph balancer ls
```

##### 11.6 放弃计划

```sh
ceph balancer rm <plan-name>
```

##### 11.7 查看当前记录的计划

```sh
ceph balancer status
```

##### 11.8 评估执行特定计划所产生的分布

```sh
ceph balancer eval <plan-name>
```

##### 11.9 执行该计划

> 如果一个计划预计会改善分布, 则可以执行

```sh
ceph balancer execute <plan-name>
```


## CRUSH MAP

#### 1. 禁止 osd 自动加入 map 

> 一般没必要禁止

```sh
vim ceph.conf
osd crush update on start = false
```

#### 2. 自定义 crush 位置

> 一般没必要自定义

```sh
vim ceph.conf
crush location hook = /etc/ceph/myrack

# 然后创建 myrack 文件
# 该钩子会向标准输出, 输出一行类似这一行内容: --cluster CLUSTER --id ID --type TYPE

vim /etc/ceph/myrack
#!/bin/sh
echo "host=$(hostname -s) rack=$(cat /etc/rack) root=default"
```

#### 3. 查看集群 crush 以及权重

```sh
ceph osd tree
```

#### 4. 查看 rule 列表

```sh
ceph osd crush rule ls
```

#### 5. 查看 rule 内容

```sh
ceph osd crush rule dump
```

#### 6. 手动设置 class

```sh
ceph osd crush set-device-class <class> <osd-name> [...]
```

#### 7. 删除 class

```sh
ceph osd crush rm-device-class <osd-name> [...]
```

#### 8. 创建针对 device class 的规则

```sh
ceph osd crush rule create-replicated <rule-name> <root> <failure-domain> <class>
```

#### 9. 应用 rule 到指定的 Pool

```sh
ceph osd pool set <pool-name> crush_rule <rule-name>
```

#### 10. 查看 shadow crush

> 会按架构的 hdd 还是 ssd 进行整理划分

```sh
ceph osd crush tree --show-shadow
```


### 权重集: `compat` 和 `per-pool`

#### 11. crush 增加 osd

```sh
ceph osd crush set {name} {weight} root={root} [{bucket-type}={bucket-name} ...]

# 示例
ceph osd crush set osd.0 1.0 root=default datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1
```

#### 12. 移除 osd

```sh
ceph osd crush remove {name}
```

#### 13. 调整 osd 权重

```sh
ceph osd crush reweight {name} {weight}
```

#### 14. 添加 bucket 

```sh
ceph osd crush add-bucket {bucket-name} {bucket-type}

# 示例
ceph osd crush add-bucket rack12 rack
```

#### 15. 将 bucket 移动到 crush 中不同位置 

```sh
ceph osd crush move {bucket-name} {bucket-type}={bucket-name}, [...]
```

#### 16. 将 bucket 从 crush 结构中删除

> bucket 从 crush 结构中删除, 该bucket必须为空, 即下面不能有任何 osd 或 其他子bucket

```sh
ceph osd crush remove {bucket-name}

# 示例
ceph osd crush remove rack12
```

#### 17. 兼容(compat)权重集

##### 17.1 创建兼容(compat)权重集

```sh
ceph osd crush weight-set create-compat
```

##### 17.2 查看权重集

```sh
ceph osd crush weight-set ls
```

##### 17.3 调整 compat 权重集的权重

```sh
ceph osd crush weight-set reweight-compat {name} {weight}
```

##### 17.4 销毁 compat 权重集

```sh
ceph osd crush weight-set rm-compat
```

#### 18. 每池(per-pool)权重集

##### 18.1 创建每池(per-pool)权重集

> 为特定池创建权重集

```sh
ceph osd crush weight-set create {pool-name} {mode}
```

##### 18.2 列出现有权重集

```sh
ceph osd crush weight-set ls
```
##### 18.3 调整权重

```sh
ceph osd crush weight-set reweight {pool-name} {item-name} {weight [...]}
```

##### 18.4 删除权重集

```sh
ceph osd crush weight-set rm {pool-name}
```

#### 19. 为复制类型 pool 创建故障域规则

```sh
ceph osd crush rule create-replicated {name} {root} {failure-domain-type} [{class}]

# 参数说明
# name: 规则名称
# root: root bucket 名称
# failure-domain-type: 故障域类型(rack, host)
# class: 要放置数据的磁盘类型(hdd,ssd,nvme,xxx), 省略表示使用所有设备
```

#### 20. 为纠删码类型 pool 创建故障域规则

> 使用纠删码必须首先创建配置文件, 配置文件中包含纠删码的规则, 故障域等等。
> 创建 pool 时, 指定纠删码配置文件
> 每个纠删码配置文件代表一种 crush 规则。所以创建新池时应该尽量创建并使用新的配置文件

##### 20.1 列出纠删码配置文件

```sh
ceph osd erasure-code-profile ls
```

##### 20.2 查看指定的纠删码配置文件

```sh
ceph osd erasure-code-profile get {profile-name}
```

##### 20.3 创建纠删码配置文件

> 纠删码配置文件由一组键值对组成
> 这些键值对中的大多数控制着对池中的数据进行编码的纠删码的行为。 
> 但是，以 CRUSH- 开头的键值对控制创建的 CRUSH 规则。

```sh
ceph osd erasure-code-profile set yangben_profile k=6 m=3 crush-failure-domain=rack crush-root=sata_root01

# 参数说明
# crush-root: 指定一个 root bucket 的名称, 数据将在这个 root 下存放, 默认: default
# crush-failure-domain: 故障域类型。 默认: host
# crush-device-class: 要放置数据的磁盘类型(hdd,ssd,nvme,xxx), 省略表示使用所有设备
# k=6 存储的数据对象将被分为6个部分
# m=3 同时丢失多少个OSD而不丢失任何数据
# l:  lrc 纠删码插件特有的参数
```

##### 20.4 创建纠删码规则

```sh
ceph osd crush rule create-erasure {name} {profile-name}

# 示例
ceph osd crush rule create-erasure sata_ec_rule1 yangben_profile
```

#### 21 删除规则

```sh
ceph osd crush rule rm {rule-name}
```

#### 22. 修改主亲和性值

```sh
# 值为 0 ~ 1 之间, 0表示该osd不能做为主, 1表示最有可能做为主

ceph osd primary-affinity <osd-id> <weight>
```

## 设备(device)

#### 1. 查看 device 列表(机器,盘符,osd编号)

> 如果启用的磁盘故障预测, 该命令可以显示预测的磁盘寿命

```sh
ceph device ls
```

#### 2. 查看指定的 osd 的机器和盘符

```sh
ceph device ls-by-daemon osd.7
```

#### 3. 查看指定机器上的 osd

```sh
ceph device ls-by-host mytestceph11v.ls.cn
```

#### 4. 根据设备ID查看具体情况

```sh
ceph device info QEMU_QEMU_HARDDISK_300d6b27-641b-44ed-8ce2-8281b6a3ecb0
```

#### 5. 使硬盘指示灯闪烁

> 可能不起作用, 与内核版本，SES 固件或 HBA 设置等因素有关系

```sh
device light on|off <devid> [ident|fault] [--force]
```

#### 6. 运行状态监控

> 好像和 mgr/prometheus 模块没有关系

```sh
ceph device monitoring on
ceph device monitoring off
```

#### 7. 设置 device 监控时间间隔

> 如果启用监控, 用这个设置抓取时间间隔，默认24小时抓取一次

```sh
ceph config set mgr mgr/devicehealth/scrape_frequency <seconds>
```

#### 8. 手动抓取所有 device 监控

```sh
ceph device scrape-health-metrics
```

#### 9. 手动抓取指定的 device 监控(device-id)

```sh
ceph device scrape-health-metrics <device-id>
```

#### 10. 手动抓取指定的 device 监控(osd.123)

```sh
ceph device scrape-daemon-health-metrics <who>
```

#### 11. 检索设备存储的运行状况指标

```sh
ceph device get-health-metrics <devid> [sample-timestamp]
```

#### 12. device 故障预测

```sh
ceph config set global device_failure_prediction_mode <mode>

# none 禁用设备故障预测
# local 使用来自 ceph-mgr 守护进程的预训练预测模型
```

#### 13. 对指定的盘强制预测

```sh
ceph device predict-life-expectancy <devid>
```

#### 14. 预期故障在指定时间间隔内故障, 则报警

```
# 时间间隔参数
mgr/devicehealth/warn_threshold
```

#### 15. 检查所有设备存储的预期寿命并生成任何适当的运行状况警报

```sh
ceph device check-health
```

#### 16. 预测故障自动迁移参数

> 如果启用该参数, 会自动将数据从预计很快会发生故障的设备中迁移出来

```
# 预测磁盘故障是否进行自动迁移
mgr/devicehealth/self_heal

# 自动迁移的时间间隔。 如果预计设备在指定时间间隔内出现故障，则会自动将其标记为淘汰。
mgr/devicehealth/mark_out_threshold

# 防止此进程级联至完全失败。 如果“自我修复”模块标记出过多的 OSD，以致超出了mon_osd_min_up_ratio 的比率值，则集群将引发 DEVICE_HEALTH_TOOMANY 健康检查

mon_osd_min_up_ratio
```

## 其他常用命令

#### 1. 集群状态

```sh
ceph -s
ceph status
```

#### 2. 集群状态和时间摘要

```sh
ceph -w
```

#### 3. mon 状态以及仲裁情况

```sh
ceph mon stat
ceph quorum_status
```

#### 4. 单个 mon 状态

```sh
ceph tell mon.[id] mon_status
```

#### 5. 为特定osd添加密钥

```sh
ceph auth add {osd} {--in-file|-i} {path-to-osd-keyring}
```

#### 6. 查看所有用户的密钥和权限

```sh
ceph auth ls
```

#### 7. 查看 pg 统计信息

```sh
ceph pg dump [--format {format}]
```

#### 8. 查看所有陷入指定状态的 PG 的统计信息

```sh
ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format {format}] [-t|--threshold {seconds}]
```

#### 9. 删除丢失的对象或将对象恢复到之前的状态

```sh
ceph pg {pgid} mark_unfound_lost revert|delete
```

#### 10. 查看 osd 状态

```sh
ceph osd stat
```

#### 11. 将最新的 osd 映射副本写入文件

```sh
ceph osd getmap -o file
```

#### 12. 将 CRUSH 映射的副本从最新的 OSD 映射写入文件

```sh
ceph osd getcrushmap -o file

# 或分两步骤实现:
ceph osd getmap -o /tmp/osdmap
osdmaptool /tmp/osdmap --export-crush file
```

#### 13. 转储 OSD map

```sh
ceph osd dump [--format {format}]
```

#### 14. 查看 osd 结构树

```sh
ceph osd map <pool-name> <object-name>
```

#### 15. 将新osd添加或移动到指定位置

```sh
ceph osd crush set {id} {weight} [{loc1} [{loc2} ...]]
```

#### 16. 从 CRUSH 映射中删除现有 OSD

```sh
ceph osd crush remove {name}
```

#### 17. 从 CRUSH 映射中删除现有 bucket

```sh
ceph osd crush remove {bucket-name}
```

#### 18. 将 bucket 从 CRUSH 层次结构中移动

```sh
ceph osd crush move {id} {loc1} [{loc2} ...]
```

#### 19. 设置 osd 权重

```sh
ceph osd crush reweight {name} {weight}
```

#### 20. 将 osd 标记为丢失

> 警告: 这可能会导致数据永久丢失。 谨慎使用！

```sh
ceph osd lost {id} [--yes-i-really-mean-it]
```

#### 21. 创建一个新 OSD

```sh
ceph osd create [{uuid}]
```

#### 22. 删除一个或多个 OSD

```sh
ceph osd rm [{id}...]
```

#### 23. 显示 osd 最大号码和epoch

```sh
ceph osd getmaxosd
```

#### 24. 设置 osd 最大参数


> [!NOTE] 注意
> 该参数的默认值为 10000。大多数操作员永远不需要调整它。


```sh
ceph osd setmaxosd
```

#### 25. 导入特定 CRUSH 映射

```sh
ceph osd setcrushmap -i file
```

#### 25. 将 OSD 标记为 down

```sh
ceph osd down {osd-num}
```


#### 26. 将 OSD 标记为 out 

```sh
ceph osd out {osd-num}
```

#### 27. 将 OSD 标记为 in 

```sh
ceph osd in {osd-num}
```

#### 28. 暂停写入

```sh
# 暂停所有IO请求
ceph osd pause

# 取消暂停
ceph osd unpause
```

#### 29. 修改权重


> [!WARNING] 警告
> 任何的修改权重都会与 balancer 冲突, 开启 balancer 就不要手动修改权重。叫所有权重都默认为 1 就好

> 与 ceph osd crush reweight 不同,  ceph osd reweight 的值只能是 0 ~ 1 之间

```sh
ceph osd reweight {osd-num} {weight}
```

#### 30. 按利用率重新加权 OSD

> 默认此命令调整平均利用率 ±20% 的 OSD 的覆盖权重

```sh
ceph osd reweight-by-utilization [threshold [max_change [max_osds]]] [--no-increasing]

# max_change: 限制任何 OSD 重新权重更改的增量, 默认 0.05
# max_osds: 限制要调整的 OSD 数量, 默认 4
# --no-increasing: osd 的权重只能减少或保持不变, 不能增加
```


#### 31.  查看哪些 pg 和 osd 将受到 reweight-by-utilization 命令的影响

```sh
ceph osd test-reweight-by-utilization [threshold [max_change max_osds]] [--no-increasing]
```

#### 32. 添加删除黑名单

```sh
# 查看黑名单
ceph osd blacklist ls

# 将IP添加到黑名单
ceph osd blocklist ["range"] add ADDRESS[:source_port][/netmask_bits] [TIME]

# 将IP从黑名单中删除
ceph osd blocklist ["range"] rm ADDRESS[:source_port][/netmask_bits]

# range: 指定一个IP段范围
# TIME: 指定加入到黑名单多少秒
```

#### 33. 创建存储池快照

```sh
ceph osd pool mksnap {pool-name} {snap-name}
ceph osd pool rmsnap {pool-name} {snap-name}
```

#### 34. 创建删除和重命名 pool 

```sh
ceph osd pool create {pool-name} [pg_num [pgp_num]]
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
ceph osd pool rename {old-name} {new-name}
```

#### 35. 查看修改池设置

```sh
ceph osd pool get {pool-name} {field}
ceph osd pool set {pool-name} {field} {value}

# field 有
# size: 副本数
# pg_num: pg 数
# pgp_num: pgp 数
# crush_rule: 映射放置的规则编号
```

#### 36. scrub 指定的 OSD

```sh
# 所有OSD用 * 表示
ceph osd scrub {osd-num}
```

#### 37. 修复 OSD

```sh
# 所有OSD用 * 表示
ceph osd repair N
```

#### 38. 对指定的OSD做吞吐量基准测试


> [!NOTE] 注意
> 该测试不是破坏性的，也不会覆盖现有的实时 OSD 数据，但可能会暂时影响同时访问 OSD 的客户端的性能



```sh
ceph tell osd.N bench [TOTAL_DATA_BYTES] [BYTES_PER_WRITE]

# TOTAL_DATA_BYTES: 增量写入总大小(默认 1G)
# BYTES_PER_WRITE: 每个请求的大小(默认 4M)
```

#### 39. 清理基准测试缓存

> 在两次基准测试之间要做清理缓存动作

```sh
ceph tell osd.N cache drop
```

#### 40. 查看缓存统计信息

```sh
ceph tell osd.N cache status
```

#### 41. 修改 mds 配置

```sh
ceph tell mds.{mds-id} config set {setting} {value}

示例:
ceph tell mds.0 config set debug_ms 1
```

#### 42. 查看 MDS 状态

```sh
ceph mds stat
```

#### 43. 将活动的 MDS 标记为故障

```sh
ceph mds fail 0
```

#### 44. 看 mon 状态信息

```sh
ceph mon stat

ceph quorum_status -f json-pretty
```

#### 45. 查看指定的 mon 节点的状态

```sh
ceph tell mon.[name] mon_status
```

#### 45. mon 转储

```sh
ceph mon dump
```