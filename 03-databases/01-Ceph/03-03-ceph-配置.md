# ceph 配置

## 常规配置

### 一. 配置的规划及说明

#### 1. 集群配置分类

> 配置分为 `守护进程配置选项` 和 `集群参数选项`

```
守护进程配置: 每个进程启动时的配置参数, 可以通过以下几种方式进行设置:
1. 监控集群的集中配置数据库
2. 本地配置文件
3. 环境变量
4. 命令行参数
5. 守护进程启动后, 执行运行时配置修改

集群参数: 集群还有一些管理整个集群状态的一些参数
集群启动后, 通过客户端命令设置
如: 
```
#### 2. 守护进程配置选项

```
# Ceph 集群有以下几类守护进程, 每一个守护进程都有许多配置选项
	Ceph Manager - 守护进程提供额外的监视并提供与外部监视和管理系统的接口。
	Ceph Monitor - 守护进程管理关键集群状态
	Ceph OSD - 对象存储守护进程
	Ceph RGW - S3 网关
	Ceph FS

# 守护进程配置方式:
1. 通过<Monitor 集中配置数据库>  -- 推荐
2. 使用每个守护进程本地的<ceph.conf 配置文件>
```

#### 3. 配置选项命名规范

```
# 小写字符 + 下划线(_) 
# 示例: mon_host

# 命令行参数: 下划线 (_) 和破折号 (-) 字符可以互换使用
# 示例: --mon-host 相当于 --mon_host

# 配置文件: 可以使用空格代替下划线或破折号
# 建议始终使用下划线
```

#### 4. 守护进程配置生效顺序

```
# 生效顺序从后到前, 即后一个配置源里的配置会覆盖前一个配置源里的相同配置选项

编译时的默认值
Monitor集中配置数据库 (建议的配置方式)
存储在本地主机上的配置文件
环境变量
命令行参数
由管理员设置的运行时覆盖

# 特殊情况可能想跳过 `监控集群的集中配置数据库`
启动进程时添加: --no-mon-config
```

#### 5. 配置文件查找顺序

```
# 配置文件查找顺序从前到后, 前一个位置找到配置文件之后会忽略后面的位置

1. $CEPH_CONF 环境变量
2. 命令行参数: -c
3. /etc/ceph/$cluster.conf
4. ~/.ceph/$cluster.conf
5. ./$cluster.conf 
6. 在 FreeBSD 系统上: /usr/local/etc/ceph/$cluster.conf
```

#### 6. 必须的引导选项

> 引导选项是`非 mon 进程`与`mon 进程`通信的配置选项
> 所以这些选项必须在节点本地配置文件
> 不能使用 `Monitor集中配置数据库` 的方式


```
# 必须配置在本地配置文件的参数:
mon_host  #  mon 节点地址
mon_host_override  # 主要用于调试, 一般不设置这个参数
mon_dns_srv_name   # dns srv 方式获取 mon 节点地址, 与 mon_host 参数作用一致, 二选一
mon_data # mon 数据目录
osd_data # osd 数据目录
mds_data # mds 数据目录
mgr_data # mgr 数据目录
keyring  # 进程与 mon 节点通信使用的 key, 默认在各进程数据目录下
keyfile
key
其他涉及到数据存储在哪个本地目录选项, 如: log_file
```

#### 7. 配置区域

> `vim /etc/ceph/ceph.conf`

```toml
[global]
debug_ms = 0

[mon]

[mon.mycephtest01v] # 单独指定守护进程

[mgr]

[osd]
debug_ms = 1

[osd.1] # 单独指定守护进程
debug_ms = 10

[mds]

[client]

# client 中的设置会影响所有 ceph 客户端, 报错
mounted 的 CephFS
mounted 的 Ceph Block Devices
RGW 进程
```

#### 8. 元变量

| 元变量名       | 含义     | 可能的值                              | 使用示例                                     |
| ---------- | ------ | --------------------------------- | ---------------------------------------- |
| `$cluster` | 集群名称   | `ceph`                            | `keyring = /etc/ceph/$cluster.keyring`   |
| `$type`    | 守护程序类型 | `mds,mon,osd`                     | `mgr_data = /var/lib/ceph/$type`         |
| `$id`      | 守护进程名称 | `osd.0 中的 0` <br>`mon.abc 中的 abc` | `/var/lib/ceph/$type/$cluster-$id`       |
| `$host`    | 主机名    | `主机名称`                            | `/var/run/ceph/$cluster-$name.asok`      |
| `$name`    | 类型.名称  | `$type.$id`                       | `/var/run/ceph/$cluster-$name.asok`      |
| `$pid`     | PID    |                                   | `/var/run/ceph/$cluster-$name-$pid.asok` |

### 二.  `Monitor集中配置数据库`

#### 1. 查看配置项的帮助信息

> `ceph config help <option>`

###### 示例

```sh
root@mytestceph11v:~# ceph config help log_file
log_file - path to log file
  (str, basic)
  Default (non-daemon): 
  Default (daemon): /var/log/ceph/$cluster-$name.log
  Can update at runtime: false
  See also: [log_to_file,log_to_stderr,err_to_stderr,log_to_syslog,err_to_syslog]

# 或
root@mytestceph11v:~# ceph config help log_file -f json-pretty

{
    "name": "log_file",
    "type": "str",
    "level": "basic",
    "desc": "path to log file",
    "long_desc": "",
    "default": "",
    "daemon_default": "/var/log/ceph/$cluster-$name.log",
    "tags": [],
    "services": [],
    "see_also": [
        "log_to_file",
        "log_to_stderr",
        "err_to_stderr",
        "log_to_syslog",
        "err_to_syslog"
    ],
    "enum_values": [],
    "min": "",
    "max": "",
    "can_update_at_runtime": false,
    "flags": []
}
```


#### 2. 掩码`MASK`

```
# 使用 ceph config dump 查看 Mon集中配置时, 输出中的第二列的列名就是 MASK 

type:location 其中 type 是 CRUSH 属性，例如机架或主机，而 location 是该属性的值。 例如，host:foo 会将该选项限制为仅适用于在特定主机上运行的守护程序或客户端。

class:device-class 其中 device-class 是 CRUSH 设备类的名称（例如 hdd 或 ssd）。 例如，class:ssd 会将选项限制为仅适用于 SSD 支持的 OSD。 （此掩码对非 OSD 守护程序或客户端没有影响。）
```
#### 3. 查看 `Mon 配置数据库` 中存储的所有配置

> `ceph config dump`

```sh
root@mytestceph11v:/data# ceph config dump
WHO     MASK  LEVEL     OPTION                              VALUE       RO
global        advanced  osd_pool_default_pg_autoscale_mode  off           
mgr           advanced  mgr/pg_autoscaler/noautoscale       true        * 
osd.0         basic     osd_mclock_max_capacity_iops_hdd    94.723856     
osd.1         basic     osd_mclock_max_capacity_iops_hdd    105.755458    
osd.12        basic     osd_mclock_max_capacity_iops_hdd    170.152197    
osd.13        basic     osd_mclock_max_capacity_iops_hdd    157.987027    
osd.14        basic     osd_mclock_max_capacity_iops_hdd    117.158200    
osd.2         basic     osd_mclock_max_capacity_iops_hdd    106.004997    
osd.3         basic     osd_mclock_max_capacity_iops_hdd    103.240996    
osd.5         basic     osd_mclock_max_capacity_iops_hdd    154.674136    
osd.6         basic     osd_mclock_max_capacity_iops_hdd    100.706252 
```

#### 4. 查看 `Mon 配置数据库` 中存储的指定进程的配置

> `ceph config get <who> [option]`

###### 示例 

```sh
# 查看 mon 的所有配置信息
[root@ossec215 ~]# ceph config get mon
WHO     MASK  LEVEL     OPTION                              VALUE  RO
mon           advanced  mon_allow_pool_delete               true
global        advanced  osd_pool_default_pg_autoscale_mode  off

# 查看 osd.0 的所有配置信息配置信息
root@mytestceph11v:~# ceph config get osd.0
WHO     MASK  LEVEL     OPTION                              VALUE      RO
osd.0         basic     osd_mclock_max_capacity_iops_hdd    94.723856    
global        advanced  osd_pool_default_pg_autoscale_mode  off          


# 查看 mon 的 mon_allow_pool_delete 参数
[root@ossec215 ~]# ceph config get mon mon_allow_pool_delete
true

# 查看 osd.0 的 osd_mclock_max_capacity_iops_hdd 配置值
root@mytestceph11v:~# ceph config get osd.0 osd_mclock_max_capacity_iops_hdd
94.723856
```

#### 5. 设置指定进程的配置到 `Mon 配置数据库`

> `ceph config set <who> <option> <value>`

> `ceph config set` 操作对应的进程实例会立即生效
> 如果本地配置文件中定义了该选项, 则设置会被忽略: `配置文件优先级高`

###### 示例

```sh
# 设置集群所有 mon 进程允许删除 pool
ceph config set mon mon_allow_pool_delete true

# 设置 osd.123 进程日志级别(0 ~ 20)
ceph config set osd.123 debug_ms 20
```

#### 6. 将指定文件的配置导入 `mon 配置库`

> `ceph config assimilate-conf -i <input file> -o <output file>`

```
# 将老版本配置导入新版本 mon 配置库时比较适用
# input file 中无效的，无法识别的配置项将输出到 output file 文件
```

#### 7. 删除指定的参数

> 示例

```sh
ceph config set mgr mgr/prometheus/stale_cache_strategy return

ceph config set mgr mgr/prometheus/scrape_interval 60
```

#### 8. 遍历删除所有 mgr 节点指定的参数

```sh
# 查看生成的删除命令
for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do echo "ceph config rm mgr.$i mgr/prometheus/scrape_interval; ceph config rm mgr.$i mgr/prometheus/stale_cache_strategy"; done;

# 执行删除
for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do ceph config rm mgr.$i mgr/prometheus/scrape_interval; ceph config rm mgr.$i mgr/prometheus/stale_cache_strategy; done;

# 查看删除后效果
for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do ceph config get mgr.$i mgr/prometheus/scrape_interval; ceph config get mgr.$i mgr/prometheus/stale_cache_strategy; done;
```

### 三. 运行时配置的查看与修改

> 修改重启后失效

#### 1. 查看守护进程当前配置

> 可以查看远程节点

> 注: 如果守护进程使用了本地配置文件 `ceph.conf`, 则 `ceph config show` 显示的配置与 ` ceph config get ` 获取到的  `Mon 配置数据库` 中的配置可能会不相同

```sh
ceph config show <who>

# 示例

# 查看 mon.mytestceph11v 进程当前生效的配置
ceph config show mon.mytestceph11v

# 查看 mon.mytestceph11v 进程当前生效的 mon_allow_pool_delete 参数的值
ceph config show mon.mytestceph11v mon_allow_pool_delete

# 查看 osd.0 所有配置
ceph config show osd.0

# 查看  osd.0 的日志级别
ceph config show osd.0 debug_osd
```
#### 2. 查看进程所有设置（包括具有默认值的设置)

> 可以查看远程节点

```sh
ceph config show-with-defaults <who>

# 示例
ceph config show-with-defaults osd.0
```

#### 3. `ceph tell` 修改运行时配置

> 可修改远程节点

> 来源显示 `override`, 并且重启失效

```sh
ceph tell <name> config set <option> <value>

# 示例

# 设置 osd.123 实例日志级别为 20
ceph tell osd.123 config set debug_osd 20

# 实例名称可以接受通配符
# 设置所有 osd 实例日志级别为 20
ceph tell osd.* config set debug_osd 20
```
#### 4. `ceph daemon` 方式查看配置帮助信息

> 在实例所在机器上本地执行

> 一般用于在集群升级时, 查看指定的实例中的配置以判断该实例是否升级完成

```sh
ceph daemon <name> config help [option]
```

#### 5. `ceph daemon` 方式查看配置

> 在实例所在机器上本地执行

```sh
# 查看 osd.0 所有配置
ceph daemon osd.0 config show

# 查看非默认设置并查看每个值的来源
ceph daemon osd.0 config diff

# 查看单个设置的值
ceph daemon osd.0 config get debug_osd
```
#### 6. `ceph daemon` 方式修改配置

> 在实例所在机器上本地执行

> 来源显示 `override`, 并且重启失效

```sh
ceph daemon <name> config set <option> <value>

# 设置 osd.4 实例日志 级别为 20
ceph daemon osd.4 config set debug_osd 20
```

#### 7. `--admin-daemon` 指定 `sock` 文件方式查看和修改配置

> 在实例所在机器上本地执行

> 来源显示 `override`, 并且重启失效

```sh
ceph --admin-daemon /var/run/ceph/xxx.asok

# 查看配置项
ceph --admin-daemon /var/run/ceph/ceph-mon.myk8snode05v.asok config show | grep mon_allow_pool_delete

# 修改配置项
ceph --admin-daemon /var/run/ceph/ceph-mon.mytestceph11v.asok config set mon_allow_pool_delete true
```

## OSD map 配置

> 有些配置需要设置到 `OSDMap` 中
### 一. 示例

#### 1. 在配置文件中的配置

> 这些设置仅在集群创建期间应用。 之后需要设置到 `OSDMap`

```toml
[global]
	mon_osd_full_ratio = .80
	mon_osd_backfillfull_ratio = .75
	mon_osd_nearfull_ratio = .70
```

#### 2. 在 `OSDMap` 中修改配置

```sh
ceph osd set-full-ratio 0.95
ceph osd set-backfillfull-ratio 0.90
ceph osd set-nearfull-ratio 0.85
```


## `pool` 参数配置

> 可以单独对指定的 pool 设置一些参数

> 查看: `ceph osd pool get <池名> <参数>`

> 修改: `ceph osd pool set <池名> <参数> <值>`

> 查看所有池默认值: `ceph osd pool get noautoscale`

> 对所有 `pool` 设置: `ceph osd pool set noautoscale`

> 对所有 `pool` 取消: `ceph osd pool unset noautoscale`

### 一. 示例

```sh
# 查看 lstest 池子的  nodelete 参数
ceph osd pool get lstest nodelete

# 修改 lstest 池子的 nodelete 参数为 true
ceph osd pool set lstest nodelete true

# 查看
ceph osd pool get noautoscale

# 设置
ceph osd pool set noautoscale

# 取消
ceph osd pool unset noautoscale
```

## 常用配置操作示例

### 一. 常规 `Mon 配置数据库` 参数示例

#### 1. `[global]` 参数

> `ceph config set global <参数名> <值>`

> 官方建议, 与心跳有关的这些参数放在 `[global]` 里

| 参数名                            | 默认值  | 说明                           |
| ------------------------------ | ---- | ---------------------------- |
| mon_osd_down_out_interval      | 10分钟 | 标记为 down 或 out 之前等待多少秒       |
| mon_osd_down_out_subtree_limit | rack | 不会自动标记的最小CRUSH单元类型，可改为: host |
| mon_osd_min_down_reporters     | 2    | osd 最小报告器                    |
| mon_osd_reporter_subtree_level | host | 子树级别                         |

#### 2. `[mon]` 参数

> `ceph config set mon <参数名> <值>`

| 参数名                            | 默认值   | 说明                               |
| ------------------------------ | ----- | -------------------------------- |
| mon_allow_pool_delete          | false | 是否允许删除 pool,  默认禁止对任何 pool 的删除操作 |
| osd_pool_default_flag_nodelete | false | 是否对新创建的 pool 开启 nodelete 标志      |
| mon_max_osd                    | 10000 | 集群允许的最大OSD数量                     |

#### 3. `[osd]` 参数

> `ceph config set osd <参数名> <值>`

| 参数名                                 | 默认值   | 说明                                                                                                                                                                                 |
| ----------------------------------- | ----- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| osd_delete_sleep                    | 0     | 下一次删除前睡眠多少秒,  防止删除操作大量占用IO影响业务性能                                                                                                                                                   |
| osd_delete_sleep_hdd                | 5     |                                                                                                                                                                                    |
| osd_delete_sleep_ssd                | 1     |                                                                                                                                                                                    |
| osd_delete_sleep_hybrid             | 1     |                                                                                                                                                                                    |
| osd_max_object_size                 | 128Mi | RADOS对象的最大大小                                                                                                                                                                       |
| osd_max_write_size                  | 90    | osd 单次写的最大大小, 单位MB, 看情况可改为 256                                                                                                                                                     |
| osd_client_message_size_cap         | 500Mi | 内存中允许的最大客户端数据消息                                                                                                                                                                    |
| osd_max_scrubs                      | 3     | OSD守护进程同时进行的最大 scrub 操作                                                                                                                                                            |
| osd_scrub_min_interval              | 1天    | 集群负载较低时, scrub 的最小间隔时间, 单位秒,                                                                                                                                                       |
| osd_scrub_max_interval              | 7天    | scrub 的最大间隔时间, 单位秒,                                                                                                                                                                |
| osd_scrub_begin_hour                | 0     | 每天几点开始 scrub, 但上一次 scrub 时间到现在超过 osd_scrub_max_interval 参数值后失效                                                                                                                     |
| osd_scrub_end_hour                  | 0     | 每天几点结束 scrub, 但上一次 scrub 时间到现在超过 osd_scrub_max_interval 参数值后失效                                                                                                                     |
| osd_scrub_begin_week_day            | 0     | 默认每周日深度 scrub, 但上一次 scrub 时间到现在超过 osd_scrub_max_interval 参数值后失效                                                                                                                    |
| osd_scrub_end_week_day              | 0     | 默认每周日深度 scrub, 但上一次 scrub 时间到现在超过 osd_scrub_max_interval 参数值后失效                                                                                                                    |
| osd_scrub_during_recovery           | false | 恢复时不 scrub                                                                                                                                                                         |
| osd_scrub_load_threshold            | 0.5   | getloadavg() /在线cpu数量 > 0.5 时不进行 scrub                                                                                                                                             |
| osd_scrub_sleep                     | 0     | scrub 每组数据之间的时间间隔                                                                                                                                                                  |
| osd_deep_scrub_interval             | 7天    | “深度” scrub (完全读取所有数据)的时间间隔。osd_scrub_load_threshold不影响此设置。                                                                                                                         |
| osd_scrub_interval_randomize_ratio  | 0.5   | 在调度PG的下一个 scrub 任务时，在osd_scrub_min_interval中增加一个随机延迟，该延迟为一个小于osd_scrub_min_interval * osd_scrub_interval_randomized_ratio的随机值。默认设置在允许的时间窗口[1,1.5]* osd_scrub_min_interval中散布 scrub |
| osd_deep_scrub_stride               | 512Ki | 深度 scrub 大小                                                                                                                                                                        |
| osd_scrub_auto_repair               | false | scrub 检测到不一致是否自动修复                                                                                                                                                                 |
| osd_scrub_auto_repair_num_errors    | 5     | 如果发现的错误超过这个数目，将不会进行自动修复                                                                                                                                                            |
| osd_op_num_shards                   | 0     | 分片数量，每个分片一个对列，pg分布在分和上。 如果不为 0 , 则后面 _hdd 和 _ssd 两个参数失效                                                                                                                            |
| osd_op_num_shards_hdd               | 5     | 同上，hdd盘                                                                                                                                                                            |
| osd_op_num_shards_ssd               | 8     | 同上,  SSD 盘                                                                                                                                                                         |
| osd_op_num_threads_per_shard        | 0     | 每个分片线程数量，非0则覆盖 _hdd 和 _ssd 的参数                                                                                                                                                     |
| osd_op_num_threads_per_shard_hdd    | 1     | 每个分片线程数量                                                                                                                                                                           |
| osd_op_num_threads_per_shard_ssd    | 2     | 每个分片线程数量                                                                                                                                                                           |
| osd_client_op_priority              | 63    | 客户端操作优先级                                                                                                                                                                           |
| osd_recovery_op_priority            | 3     | 恢复优先级                                                                                                                                                                              |
| osd_scrub_priority                  | 5     | scrub 优先级                                                                                                                                                                          |
| osd_requested_scrub_priority        | 120   | 用户在工作队列上请求擦洗的优先级集。如果这个值小于osd_client_op_priority，那么当scrub阻塞客户端时，可以将其提升到osd_client_op_priority的值                                                                                     |
| osd_snap_trim_priority              | 5     | 为快照修剪工作队列设置的优先级                                                                                                                                                                    |
| osd_op_thread_timeout               | 15    | 操作线程超时，单位为秒。                                                                                                                                                                       |
| osd_max_backfills                   | 1     | 允许回填到单个 OSD 或从单个 OSD 回填的最大数量。 请注意，这对于读取和写入操作是分开应用的。                                                                                                                                |
| osd_backfill_scan_min               | 64    | 每次回填扫描的最小对象数。                                                                                                                                                                      |
| osd_backfill_scan_max               | 512   | 每个 OSD 在一次数据回填操作中最多扫描的 PG 数量                                                                                                                                                       |
| osd_backfill_retry_interval         | 30    | 重试回填请求之前等待的秒数                                                                                                                                                                      |
| osd_recovery_delay_start            | 0     | peering 之后，延迟多少秒再开始恢复                                                                                                                                                              |
| osd_recovery_max_active             | 0     | 每个 OSD 一次的活动恢复请求数。 更多请求将加速恢复，但请求会增加集群的负载。非0时, hdd 和 ssd 失效                                                                                                                         |
| osd_recovery_max_active_hdd         | 3     | 每个 OSD 一次的活动恢复请求数                                                                                                                                                                  |
| osd_recovery_max_active_ssd         | 10    | 每个 OSD 一次的活动恢复请求数                                                                                                                                                                  |
| osd_recovery_max_chunk              | 8Mi   | 恢复操作数据块最大大小                                                                                                                                                                        |
| osd_recovery_max_single_start       | 1     | 每个osd新启动的恢复操作数量                                                                                                                                                                    |
| osd_recovery_sleep                  | 0     | 两个恢复操作之前的休眠时间                                                                                                                                                                      |
| osd_recovery_sleep_hdd              | 0.1   |                                                                                                                                                                                    |
| osd_recovery_sleep_ssd              | 0.0   |                                                                                                                                                                                    |
| osd_recovery_sleep_hybrid           | 0.025 |                                                                                                                                                                                    |
| osd_recovery_priority               | 5     |                                                                                                                                                                                    |
| osd_fast_fail_on_connection_refused | true  | OSD down 立即标记为 down.                                                                                                                                                               |

### 二. `OSDMap` 参数示例

> `ceph osd <set-参数名> <值>`

| 参数名                    | 默认值  | 说明                        |
| ---------------------- | ---- | ------------------------- |
| set-full-ratio         | 0.95 | 每个 OSD 最大可用空间为 95%        |
| set-backfillfull-ratio | 0.90 | OSD 空间超过 90% 不进行 backfill |
| set-nearfull-ratio     | 0.85 | 空间到 85% 报警                |

### 三. `pool` 参数示例

> `ceph osd pool set <池名> <参数> <值>`

| 参数名      | 默认值                                       | 说明            |
| -------- | ----------------------------------------- | ------------- |
| nodelete | 跟随: mon.osd_pool_default_flag_nodelete 参数 | 是否允许删除当前 pool |


#### 操作配置回填速度

```sh
ceph tell osd.* injectargs --osd_map_cache_size=50 --osd_max_backfills=5 --osd_recovery_max_active=15 --osd_recovery_max_single_start=10 --osd_backfill_scan_max=512
```