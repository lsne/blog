
#### ceph 配置文件

```toml
[global]
auth_client_required = cephx
auth_cluster_required = cephx
auth_service_required = cephx
auth_supported = cephx
cluster_network = 10.49.148.34/27
public_network = 10.49.148.34/27
fsid = 3438762e-ab72-11ed-bac1-78aa8234b2d1
mon_max_pg_per_osd = 600
mon_osd_down_out_interval = 8640000
osd_pool_default_crush_rule = -1
osd_pool_default_min_size = 1
osd_pool_default_size = 1
mon_host = [v2:10.49.148.34:3300/0,v1:10.49.148.34:6789/0] [v2:10.49.148.35:3300/0,v1:10.49.148.35:6789/0] [v2:10.49.148.36:3300/0,v1:10.49.148.36:6789/0]
container_image = ecr-sh.yun.lsne.cn/ceph/ceph14-lsne-daemon:v0.9.1

[mgr]
mgr_stats_threshold = 0

[osd]
bdev_async_discard = True
bdev_enable_discard = True
bluestore_block_db_size = 161061273600
osd_client_message_cap = 10000
osd_enable_op_tracker = False
osd_heartbeat_grace = 60
osd_heartbeat_interval = 15
osd_journal_size = 20480
osd_max_backfills = 1
osd_op_num_shards_hdd = 32
osd_op_num_threads_per_shard_ssd = 1
osd_op_thread_suicide_timeout = 600
osd_op_thread_timeout = 580
osd_op_threads = 10
osd_recovery_max_active = 3
osd_recovery_max_single_start = 1
osd_recovery_thread_suicide_timeout = 600
osd_recovery_thread_timeout = 580
osd_scrub_begin_hour = 2
osd_scrub_end_hour = 6
osd_scrub_load_threshold = 5
osd_scrub_sleep = 2
osd_scrub_thread_suicide_timeout = 600
osd_scrub_thread_timeout = 580
rocksdb_perf = True

osd_pg_object_context_cache_count = 1000000
```

#### 标准的s3 ceph 配置文件

```toml
[client]
rbd_cache_max_dirty = 100663296
rbd_cache_max_dirty_age = 5
rbd_cache_size = 134217728
rbd_cache_target_dirty = 67108864
rbd_cache_writethrough_until_flush = True
rgw_enable_usage_log = true

[client.rgw.oss01.rgw0]
host = oss01
keyring = /var/lib/ceph/radosgw/ceph-rgw.oss01.rgw0/keyring
rgw_frontends = civetweb port=443s  ssl_certificate=/etc/ceph/certs/_.yun.lsne.com_chained.crt
log file = /var/log/ceph/ceph-rgw-oss01.rgw0.log
rgw_thread_pool_size = 512
rgw_dynamic_resharding = false
rgw_s3_auth_use_rados = true
rgw_s3_auth_use_keystone = false
rgw_s3_auth_aws4_force_boto2_compat = false
rgw_s3_auth_use_ldap = false
rgw_s3_success_create_obj_status = 0
rgw_relaxed_s3_bucket_names = false
rgw_s3_auth_use_sts = true
rgw_cache_expiry_interval = 3600
rgw op thread suicide timeout = 30

[client.rgw.oss03.rgw1]
host = oss03
keyring = /var/lib/ceph/radosgw/ceph-rgw.oss03.rgw1/keyring
log file = /var/log/ceph/ceph-rgw-oss03.rgw1.log
rgw_frontends = civetweb port=7480 
rgw_thread_pool_size = 512
rgw_dynamic_resharding = false
rgw_s3_auth_use_rados = true
rgw_s3_auth_use_keystone = false
rgw_s3_auth_aws4_force_boto2_compat = false
rgw_s3_auth_use_ldap = false
rgw_s3_success_create_obj_status = 0
rgw_relaxed_s3_bucket_names = false
rgw_s3_auth_use_sts = true
rgw_cache_expiry_interval = 3600
rgw_op_thread_suicide_timeout = 30

# Please do not change this file directly since it is managed by Ansible and will be overwritten
[global]
auth client required = none
auth cluster required = none
auth service required = none
auth supported = none
cluster network = 0.0.0.0/0
fsid = 686a725d-317f-431d-bca7-76dbcd7b9b0a
mon host = [v2:10.46.153.147:3300,v1:10.46.153.147:6789],[v2:10.46.153.148:3300,v1:10.46.153.148:6789],[v2:10.46.153.149:3300,v1:10.46.153.149:6789]
mon initial members = oss01,oss02,oss03
mon_max_pg_per_osd = 600
mon_osd_down_out_interval = 8640000
osd pool default crush rule = -1
osd_pool_default_min_size = 1
osd_pool_default_size = 3
public network = 0.0.0.0/0

[mds]
mds cache memory limit = 64424509440
mds session autoclose = 30
mds standby for rank = 0
mds standby replay = True
mds_max_purge_files = 10240
mds_max_purge_ops_per_pg = 1024
ms_async_op_threads = 10

[mgr]
mgr_stats_threshold = 0

[osd]
bdev_async_discard = True
bdev_enable_discard = True
bluestore_block_db_size = 107374182400
bluestore_min_alloc_size_hdd = 262144
filestore_commit_timeout = 3000
filestore_fd_cache_size = 2500
filestore_max_inline_xattr_size = 254
filestore_max_inline_xattrs = 6
filestore_max_sync_interval = 10
filestore_op_thread_suicide_timeout = 600
filestore_op_thread_timeout = 580
filestore_op_threads = 10
filestore_queue_max_bytes = 1048576000
filestore_queue_max_ops = 50000
filestore_wbthrottle_enable = False
journal_max_write_bytes = 1048576000
journal_max_write_entries = 1000
journal_queue_max_bytes = 1048576000
journal_queue_max_ops = 3000
max_open_files = 327680
osd_client_message_cap = 10000
osd_enable_op_tracker = False
osd_heartbeat_grace = 60
osd_heartbeat_interval = 15
osd_journal_size = 20480
osd_max_backfills = 25
osd_op_num_shards_hdd = 32
osd_op_num_threads_per_shard_ssd = 1
osd_op_thread_suicide_timeout = 600
osd_op_thread_timeout = 580
osd_op_threads = 10
osd_recovery_max_active = 25
osd_recovery_max_single_start = 25
osd_recovery_thread_suicide_timeout = 600
osd_recovery_thread_timeout = 580
osd_scrub_begin_hour = 2
osd_scrub_end_hour = 6
osd_scrub_load_threshold = 5
osd_scrub_sleep = 2
osd_scrub_thread_suicide_timeout = 600
osd_scrub_thread_timeout = 580
rocksdb_perf = True
osd_memory_target = 10737418240 
osd_memory_base = 2147483648
osd_memory_cache_min = 3195011072
```

#### 标准的 rbd ceph 配置文件

```toml
[client]
rbd_cache_max_dirty = 100663296
rbd_cache_max_dirty_age = 5
rbd_cache_size = 134217728
rbd_cache_target_dirty = 67108864
rbd_cache_writethrough_until_flush = True
 
# Please do not change this file directly since it is managed by Ansible and will be overwritten
[global]
auth client required = none
auth cluster required = none
auth service required = none
auth supported = none
cluster network = 10.48.23.137/26
fsid = f1908a11-aa42-42b6-b01f-e612a8ede6ee
mon_max_pg_per_osd = 600
mon_osd_down_out_interval = 8640000
osd pool default crush rule = -1
osd_pool_default_min_size = 1
osd_pool_default_size = 3
public network = 10.48.23.137/26
 
[mgr]
mgr_stats_threshold = 0
 
[osd]
bdev_async_discard = True
bdev_enable_discard = True
bluestore_block_db_size = 161061273600
bluefs_shared_alloc_size = 262144
bluestore_min_alloc_size_hdd = 262144
filestore_commit_timeout = 3000
filestore_fd_cache_size = 2500
filestore_max_inline_xattr_size = 254
filestore_max_inline_xattrs = 6
filestore_max_sync_interval = 10
filestore_op_thread_suicide_timeout = 600
filestore_op_thread_timeout = 580
filestore_op_threads = 10
filestore_queue_max_bytes = 1048576000
filestore_queue_max_ops = 50000
filestore_wbthrottle_enable = False
journal_max_write_bytes = 1048576000
journal_max_write_entries = 1000
journal_queue_max_bytes = 1048576000
journal_queue_max_ops = 3000
max_open_files = 327680
osd memory target = 10737418240
osd_client_message_cap = 10000
osd_enable_op_tracker = False
osd_heartbeat_grace = 60
osd_heartbeat_interval = 15
osd_journal_size = 20480
osd_max_backfills = 1
osd_op_num_shards_hdd = 32
osd_op_num_threads_per_shard_ssd = 1
osd_op_thread_suicide_timeout = 600
osd_op_thread_timeout = 580
osd_op_threads = 10
osd_recovery_max_active = 3
osd_recovery_max_single_start = 1
osd_recovery_thread_suicide_timeout = 600
osd_recovery_thread_timeout = 580
osd_scrub_begin_hour = 2
osd_scrub_end_hour = 6
osd_scrub_load_threshold = 5
osd_scrub_sleep = 2
osd_scrub_thread_suicide_timeout = 600
osd_scrub_thread_timeout = 580
rocksdb_perf = True
```