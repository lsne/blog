# ceph 运维操作

## 运维 ceph 常用命令

### 一. ceph 参数调整

#### 调整 osd_max_backfills

> 控制每个 OSD 在同时进行的数据回填操作的最大数量

```sh
ceph tell osd.* injectargs --osd_max_backfills 20
```

#### 调整均衡速度

```sh
ceph daemon osd.3  config set osd_recovery_max_omap_entries_per_chunk 16
```

#### 禁用 OSD 的两种数据一致性检查

```sh
ceph osd set noscrub
ceph osd set nodeep-scrub
```

#### 查看 pgp 设置的目标值

> 有时候数据均衡导致 pgp 设置之后不生效, 可以使用这个命令查看

```sh
ceph osd dump|less
```

#### 查看不 clean 的 pg 状态

```sh
ceph pg dump_stuck unclean
```

---

## ceph 命令

### 一. 全局状态

#### 1. 查看集群状态

```sh
ceph -s
```

#### 2. 实时刷新监控集群状态

```sh
ceph -w
```

#### 3. 查看健康

```sh
ceph health detail
```

#### 4. 查看集群仲裁情况

```sh
ceph quorum_status
```

#### 5. osd 类型

##### 5.1 查看当前都有哪些 class

```sh
ceph osd crush class ls
```

##### 5.2 手动创建 class

> 一般不用手动创建, 给 osd 设置一个新的 class 时, 会自动创建

```sh
ceph osd crush class create nvme
```

#### 6. 指定 mon socket 查看参数

```sh
ceph --admin-daemon /var/run/ceph/ceph-mon.myk8snode05v.asok help

# 查看 monitor 进程的配置参数
ceph --admin-daemon /var/run/ceph/ceph-mon.myk8snode05v.asok config show | grep clock

# 调整参数
ceph --admin-daemon /var/run/ceph/ceph-mon.myk8snode05v.asok config set mon_clock_drift_allowed 1.0
```

#### 7. 查看 crash 相关

```sh
# 查看 crash 信息
ceph crash ls


# 查看 crash 详情
ceph crash info 2023-10-15-xxxxxxxxxxxxxxxxxxxx

# 对 carsh 信息打包归档

ceph crash archive 2023-10-15-xxxxxxxxxxxxxxxxxxxx
```


### 二. 数据一致性检查

> 系统默认设置针对 pg 做

#### 1. scrub 轻量检查

> 每天做
> 只检查元数据: 文件名, 文件属性, 文件大小等
> 对磁盘有坏道等问题可以检查出来

#### 2. deeper scrubbing 深度检查

> 默认好像通常每周一次
> 对比数据内容, 对所有数据做 check sum
> 对于上百T的大集群, 深度检查将非常消耗资源

#### 3. 查看集群 scrub 信息

```sh
ceph -h | grep scrub 
```

#### 4. 可以手动执行检查

```sh
# 查看 pg id 等信息
ceph pg dump

# 对 pg 1.6e 做检查
ceph pg scrub 1.6e

# 对 pg 1.6e 做深度检查, 会消耗大量内存,cpu等资源
ceph pg deep-scrub 1.6e
```

### 三. mgr 管理

#### 1. 手动将 mgr 节点标记为失败状态

```sh
ceph mgr failure <mgr name>
```

#### 2. 查看模块信息

```sh
ceph mgr module ls
```

#### 3. 启用 rgw 模块

```sh
ceph mgr module enable rgw
```

### 四. pool 管理

#### 1. 创建 pool

```sh
# ceph osd pool create <名称> <pg数量> <pgp数量> 
ceph osd pool create lstest 64 64

# 还可以指定副本数量， 使用纠删码等参数
```

#### 2. 查看 pool

```sh
ceph osd lspools

rados lspools

# 查看使用量, 使用大小是三副本相加后的大小
rados df
```

#### 3. 查看 pool 参数, 修改 pool 参数

```sh
# 查看和修改副本数量
ceph osd pool get lstest size
ceph osd pool set lstest size 2

# 查看和修改 pg 和 pgp 数量
ceph osd pool get lstest pg_num
ceph osd pool get lstest pgp_num
ceph osd pool set lstest pg_num 128
ceph osd pool set lstest pgp_num 128

# 查看 pool 的调度算法
ceph osd pool get lstest crush_rule
```

#### 4. 查看 pool 应用类型

```sh
ceph osd pool application get lstest
```

#### 5. 设置 pool 应用类型

```sh
# [cephfs,rbd,rgw] 三种类型可选
ceph osd pool application enable lstest rbd

# 或直接初始化为 rbd 类型
rbd pool init lstest
```

#### 6. 删除 pool

```sh
# 查看允许删除参数
ceph --admin-daemon /var/run/ceph/ceph-mon.myk8snode05v.asok config show | grep mon_allow_pool_delete

# 设置允许删除参数为 true

# 单节点设置, 需要在所有 monitor 节点都执行, 并且是临时生效。 monitor 重启后失效
ceph --admin-daemon /var/run/ceph/ceph-mon.myk8snode05v.asok config set mon_allow_pool_delete true

# 第二种设置参数的方法, 编辑 admin 节点配置文件:
[global]
mon_allow_pool_delete = true
# 然后推送到所有节点
ceph-deploy --overwrite-conf config push myk8snode05v.cpp.zzt.lsne.cn myk8snode06v.cpp.zzt.lsne.cn myk8snode07v.cpp.zzt.lsne.cn
# 然后三台重启 monitor 服务
systemctl restart ceph-mon@1

# 删除 pool (必须写两次要删除的 pool 名称)
ceph osd pool rm pool001 pool001 --yes-i-really-really-mean-it
```

### 五. OSD 操作

#### 1. 查看 osd 对应该的机器, 盘符

```sh
ceph osd metadata |less
```

#### 2. 查看逻辑卷对应该的 osd

```sh
# 在 osd 机器上执行

ceph-volume lvm list
```

#### 3. 查看 osdmap

```sh
ceph osd getmap -o osd.map
```

#### 4. 生成 pg 重新映射 osd 的命令

```sh
osdmaptool osd.map --upmap out.txt --upmap-pool cinder.volumes.ssd --upmap-max=100
```

#### 5. 生成的命令如下:

```sh
ceph osd pg-upmap-items 2.2b8a 224 260
ceph osd pg-upmap-items 2.2bbb 316 64
ceph osd pg-upmap-items 2.2bec 287 331
ceph osd pg-upmap-items 2.2bef 224 281 49 375
ceph osd pg-upmap-items 2.2bf7 328 353
ceph osd pg-upmap-items 2.2c0b 235 38 287 282
ceph osd pg-upmap-items 2.2c15 401 402
ceph osd pg-upmap-items 2.2c2c 399 346
ceph osd pg-upmap-items 2.2c2d 328 33
```

### 六. 重分布 rebalancing 

#### 1. 扩容分为

> 横向扩容: 增加机器: scale out
> 纵向扩容: 增加磁盘: scale up

#### 2. 扩容影响

> 扩容 osd 会自动触发 rebalancing
> 磁盘坏掉大概10分钟后也会触发重分布
> 一次性扩容多个 osd 会导致大量数据迁移, 影响业务
> 一次最好扩容一个osd, 等数据迁移完成, 再扩容第二个osd

#### 3. 每个osd做数据均衡的线程数量

> 数量越多, 均衡速度越快, 但消耗磁盘IO越多, 会影响业务

```sh
[root@myk8snode05v lsne]# ceph --admin-daemon /var/run/ceph/ceph-mon.myk8snode05v.asok config show | grep osd_max_backfills
    "osd_max_backfills": "1",
```

#### 4. 关闭 rebalance

> 如果扩容 osd 导致影响业务, 需要关闭 rebalance, 关闭 backfill

##### 4.1 查看参数

```sh
[lsne@myk8smaster04v cephcluster]$ ceph -h | grep norebalance
 norebalance|norecover|noscrub|nodeep-scrub|
 norebalance|norecover|noscrub|nodeep-scrub|
 
[lsne@myk8smaster04v cephcluster]$ ceph -h | grep nobackfill
osd set full|pause|noup|nodown|noout|noin|nobackfill|   set <key>
osd unset full|pause|noup|nodown|noout|noin|nobackfill| unset <key>
```

##### 4.2 设置参数

```sh
ceph osd set norebalance
ceph osd set nobackfill
```

##### 4.3 查看集群状态

```sh
[lsne@myk8smaster04v cephcluster]$ ceph -s
  cluster:
    id:     85abe2d2-f5bf-4371-ab87-00e10cf9a7a8
    health: HEALTH_WARN
            nobackfill,norebalance flag(s) set
```

##### 4.4 取消设置参数

```sh
ceph osd unset norebalance
ceph osd unset nobackfill
```

##### 4.5 osd 常用参数

```
noup：OSD启动时，会将自己在MON上标识为UP状态，设置该标志位，则OSD不会被自动标识为up状态

nodown：OSD停止时，MON会将OSD标识为down状态，设置该标志位，则MON不会将停止的OSD标识为down状态，设置noup和nodown可以防止网络抖动

noout：设置该标志位，则mon不会从crush映射中删除任何OSD。对OSD作维护时，可设置该标志位，以防止CRUSH在OSD停止时自动重平衡数据。OSD重新启动时，需要清除该flag

noin：设置该标志位，可以防止数据被自动分配到OSD上

norecover：设置该flag，禁止任何集群恢复操作。在执行维护和停机时，可设置该flag

nobackfill：禁止数据回填

noscrub：禁止清理操作。清理PG会在短期内影响OSD的操作。在低带宽集群中，清理期间如果OSD的速度过慢，则会被标记为down。可以该标记来防止这种情况发生

nodeep-scrub：禁止深度清理

norebalance：禁止重平衡数据。在执行集群维护或者停机时，可以使用该flag

pause：设置该标志位，则集群停止读写，但不影响osd自检

full：标记集群已满，将拒绝任何数据写入，但可读
```

### 七. PG 操作

#### 1. PG 状态

```
Creating：PG正在被创建。通常当存储池被创建或者PG的数目被修改时，会出现这种状态

Active：PG处于活跃状态。可被正常读写

Clean：PG中的所有对象都被复制了规定的副本数

Down：PG离线

Replay：当某个OSD异常后，PG正在等待客户端重新发起操作

Splitting：PG正在初分割，通常在一个存储池的PG数增加后出现，现有的PG会被分割，部分对象被移动到新的PG

Scrubbing：PG正在做不一致校验

Degraded：PG中部分对象的副本数未达到规定数目

Inconsistent：PG的副本出现了不一致。如果出现副本不一致，可使用ceph pg repair来修复不一致情况

Peering：Perring是由主OSD发起的使用存放PG副本的所有OSD就PG的所有对象和元数据的状态达成一致的过程。Peering完成后，主OSD才会接受客户端写请求

Repair：PG正在被检查，并尝试修改被发现的不一致情况

Recovering：PG正在迁移或同步对象及副本。通常是一个OSD down掉之后的重平衡过程

Backfill：一个新OSD加入集群后，CRUSH会把集群现有的一部分PG分配给它，被称之为数据回填

Backfill-wait：PG正在等待开始数据回填操作

Incomplete：PG日志中缺失了一关键时间段的数据。当包含PG所需信息的某OSD不可用时，会出现这种情况

Stale：PG处理未知状态。monitors在PG map改变后还没收到过PG的更新。集群刚启动时，在Peering结束前会出现该状态

Remapped：当PG的acting set变化后，数据将会从旧acting set迁移到新acting set。新主OSD需要一段时间后才能提供服务。因此这会让老的OSD继续提供服务，直到PG迁移完成。在这段时间，PG状态就会出现Remapped
```

### 八. crush rule

-  在对集群做 osd 扩容,下线, 或者修改规则修改class等操作时, 尽可能在每次操作之前将规则的 bin 文件 dump下来并保存
-  规则一定要在 pool 创建的时候就规划好, 否则会涉及到大量数据迁移
-  一定要修改配置文件: [osd].osd_crush_update_on_start = false
-  否则一但osd重启,将会使用默认 crush rule 。导致手动修改的 rule 失效。集群故障

#### 1. 修改配置文件

```sh
vim ceph.conf
[osd]
osd crush update on start = false

ceph-deploy --overwrite-conf config push myk8snode08v.cpp.zzt.lsne.cn myk8snode09v.cpp.zzt.lsne.cn myk8snode010v.cpp.zzt.lsne.cn

systemctl restart ceph-osd.target
```

#### 2. 查看数据分布

```sh
ceph osd crush tree

或

ceph osd tree
```

#### 3. 查看 crush rule 名称列表

```sh
ceph osd crush rule ls
```

#### 4. 查看详细规则

```sh
ceph osd crush dump
```

#### 5. 修改规则: dump规则文件方式修改规则

##### 5.1 获取二进制规则文件

```sh
ceph osd getcrushmap -o crushmap.bin
```

##### 5.2 将二进制规则文件编译成文本文件

```sh
crushtool -d crushmap.bin -o crushmap.txt
```

##### 5.3 编辑文本文件

```sh
vim crushmap.txt
```

###### 5.3.1 将每台机器上第一块盘的osd class 改为 ssd

```sh
# devices
device 0 osd.0 class ssd
device 1 osd.1 class hdd
device 2 osd.2 class hdd
device 3 osd.3 class hdd
device 4 osd.4 class ssd
device 5 osd.5 class hdd
device 6 osd.6 class hdd
device 7 osd.7 class hdd
device 8 osd.8 class ssd
device 9 osd.9 class hdd
device 10 osd.10 class hdd
device 11 osd.11 class hdd
```

###### 5.3.2 删除 默认host中以上修改为ssd的 osd

```sh
host myk8snode08v {
        id -3           # do not change unnecessarily
        id -4 class hdd         # do not change unnecessarily
        # weight 0.156
        alg straw2
        hash 0  # rjenkins1
        item osd.1 weight 0.039
        item osd.2 weight 0.039
        item osd.3 weight 0.039
}
host myk8snode09v {
        id -5           # do not change unnecessarily
        id -6 class hdd         # do not change unnecessarily
        # weight 0.156
        alg straw2
        hash 0  # rjenkins1
        item osd.5 weight 0.039
        item osd.6 weight 0.039
        item osd.7 weight 0.039
}
host myk8snode10v {
        id -7           # do not change unnecessarily
        id -8 class hdd         # do not change unnecessarily
        # weight 0.156
        alg straw2
        hash 0  # rjenkins1
        item osd.9 weight 0.039
        item osd.10 weight 0.039
        item osd.11 weight 0.039
}
```

###### 5.3.3 添加以下 3台 ssd 类型 host 

```sh
# 每个 host 中的两个 id 字段省略, 集群会自动分配

host myk8snode08v-ssd {
        # weight 0.156
        alg straw2
        hash 0  # rjenkins1
        item osd.0 weight 0.039
}
host myk8snode09v-ssd {
        # weight 0.156
        alg straw2
        hash 0  # rjenkins1
        item osd.4 weight 0.039
}
host myk8snode10v-ssd {
        # weight 0.156
        alg straw2
        hash 0  # rjenkins1
        item osd.8 weight 0.039
}
```

###### 5.3.4 添加 root 规则

```sh
root ssd {
        # weight 0.469
        alg straw2
        hash 0  # rjenkins1
        item myk8snode08v-ssd weight 0.049
        item myk8snode09v-ssd weight 0.049
        item myk8snode10v-ssd weight 0.049
}

```

###### 5.3.5 添加 rule 规则

```sh
# 这里的 id 必须手动指定
rule ssd_rule {
        id 10
        type replicated
        min_size 1
        max_size 10
        step take ssd
        step chooseleaf firstn 0 type host
        step emit
}
```

##### 5.4 编译为二进制文件

```sh
crushtool -c crushmap.txt -o crushmap-new.bin
```

##### 5.5 应用规则

```sh
ceph osd setcrushmap -i crushmap-new.bin
```

##### 5.6 查看 osd 树

```sh
ceph osd tree

# 可以看到有两个规则, 一个是 root default; 一个是 root ssd
```

##### 5.7 修改指定 pool 的规则为这个新规则

```sh
# 查看当前所有 pool 
 rados lspools
 
# 查看 pool001 当前规则
ceph osd pool get pool001 crush_rule

# 查看集群所有可用规则
ceph osd crush rule ls

# 给 pool001 设置为 ssd_rule 规则
ceph osd pool set pool001 crush_rule ssd_rule
 
# 查看结果，验证
ceph osd pool get pool001 crush_rule
```

##### 5.8 测试

```sh
# 在 pool001 里创建一个 rbd 文件
rbd create pool001/testcrush.img --size 10G

# 查看文件存放结构。 可以看到三个副本分别落到了 4,8,0 三个osd上
ceph osd map pool001 testcrush.img
```

#### 6. 修改规则: 命令行方式

##### 6.1 创建 root bucket

```sh
ceph osd crush add-bucket nvme root


# 查看
ceph osd tree
```

##### 6.2  创建 host bucket

```sh
ceph osd crush add-bucket myk8snode08v-nvme host
ceph osd crush add-bucket myk8snode09v-nvme host
ceph osd crush add-bucket myk8snode10v-nvme host

# 查看
ceph osd tree
```

##### 6.3 移动 host bucket 到 root bucket 下

```sh
ceph osd crush move myk8snode08v-nvme root=nvme
ceph osd crush move myk8snode09v-nvme root=nvme
ceph osd crush move myk8snode10v-nvme root=nvme

# 查看
ceph osd tree
```

##### 6.4 移动指定的 osd 到对应的 host bucket 下(注意osd与机器名要对应)

```sh
ceph osd crush move osd.3 host=myk8snode08v-nvme root=nvme
ceph osd crush move osd.7 host=myk8snode09v-nvme root=nvme
ceph osd crush move osd.11 host=myk8snode10v-nvme root=nvme

# 查看
ceph osd tree
```

##### 6.5 修改 osd 标记的 class

```sh
# 删除 class
ceph osd crush rm-device-class 3 7 11

# 添加 class
ceph osd crush set-device-class nvme 3 7 11
```

##### 6.6 定义规则

```sh
# ceph osd crush rule create-replicated <规则名称> <root bucket> <容灾机制> <class>

ceph osd crush rule create-replicated nvme_rule nvme host nvme
```

##### 6.7 查看规则

```sh
ceph osd crush rule dump
ceph osd crush rule ls
```

##### 6.8 删除规则

```sh
ceph osd crush rule rm nvme_rule
```

##### 6.9 应用规则

```sh
# 创建 pool 的同时指定规则
ceph osd pool create pool002 64 64 nvme_rule

# 或者先创建 pool 然后再修改规则
ceph osd pool create pool002 64 64 
ceph osd pool set pool002 crush_rule ssd_rule
 
# 查看结果，验证
ceph osd pool get pool002 crush_rule
```

##### 6.10 测试

```sh
# 在 pool001 里创建一个 rbd 文件
rbd create pool002/testcrush.img --size 10G

# 查看文件存放结构。 可以看到三个副本分别落到了 4,8,0 三个osd上
ceph osd map pool002 testcrush.img
```

#### 7. 常用规则命令

##### 7.1 查看规则列表

```sh
ceph osd crush rule ls
```

##### 7.2 查看规则详细, 规则id等信息

```sh
ceph osd crush rule dump | less
```

##### 7.3 查看规则

### 九 ceph ls 命令

#### 1. 创建 meta pool 和 data pool

```sh
ceph osd pool create mymeta 16 16
ceph osd pool create mydata 16 16
```

#### 2. 创建文件系统

```sh
ceph fs new myfs mymeta mydata
```

#### 3. 查看文件系统

```sh
 ceph fs ls
```

#### 4. 通过内核挂载(使用到 mount.ceph 性能比较高)

```sh
mount -t ceph -o name=admin, secret=AQAinltT8Ip9AhAAS93FrXLrrnVp8/sQhjvTIg== myk8snode05v.cpp.zzt.lsne.cn:6789:/ /lstest3/

# 或
echo "AQAinltT8Ip9AhAAS93FrXLrrnVp8/sQhjvTIg==" > /data1/ceph/adminkey
mount -t ceph -o name=admin, secretfile=/data1/ceph/adminkey myk8snode05v.cpp.zzt.lsne.cn:6789:/ /lstest3/

# 写到 /etc/fstab
myk8snode05v.cpp.zzt.lsne.cn:6789:/ /lstest3/ ceph name=admin, secretfile=/data1/ceph/adminkey, noatime 0 2
```

#### 5. 在 ceph 集群中有多个 fs 的情况下, 挂载一个非默认的

```sh
mount -t ceph :/ /mnt/mycephfs -o name=admin,fs=cephfs2

or

mount -t ceph :/ /mnt/mycephfs -o name=admin,mds_namespace=cephfs2
```

#### 5. 通过 FUSE 挂载

```sh
ceph-fuse -n client.admin -m myk8snode05v.cpp.zzt.lsne.cn:6789,myk8snode06v.cpp.zzt.lsne.cn:6789,myk8snode07v.cpp.zzt.lsne.cn:6789 /lstest3/
```

## rbd 命令

### 一. 常用命令

#### 1. 性能测试

> 官方提供的 rbd bench 测试命令
> 或者可使用第三方测试命令 fio

```sh
# 顺序写
rbd bench lstest/test.img --io-size 1M --io-threads 16 --io-total 200M --io-pattern seq --io-type write

# 随机写
rbd bench lstest/test.img --io-size 4k --io-threads 16 --io-total 200M --io-pattern rand --io-type write

# 随机读
rbd bench lstest/test.img --io-size 4k --io-threads 16 --io-total 200M --io-pattern rand --io-type read

# 随机读写
rbd bench lstest/test.img --io-size 4k --io-threads 16 --io-total 200M --io-pattern rand --io-type readwrite --rw-mix-read 70
```


#### 2. 初始化 rbd pool

```sh
rbd pool init lstest
```

#### 3. 创建 rbd 文件

```sh
rbd create -p lstest --image lsrbd1.img --size 10G

# 或
rbd create lstest/lsrbd2.img --size 10G

# 创建一个带快照功能的 rbd 文件
rbd create lstest/lsrbd3.img --image-feature layering --size 10G
```

#### 4. 查看指定 pool 中的 rbd

```sh
rbd -p lstest ls
```

### 5. 查看 rbd 信息

```sh
rbd info lstest/lsrbd1.img
```

#### 5. 删除 rbd

```sh
rbd rm -p lstest --image lsrbd1.img

或

rbd rm lstest/lsrbd2.img
```

#### 6. 挂载 rbd 到系统

```sh
# 如果在 centos7.x 挂载失败并报错: RBD image feature set mismatch 则需要执行这个命令关闭一些特性
rbd feature disable lstest/lsrbd1.img object-map fast-diff deep-flatten

# 挂载为系统的 /dev/rbd0
rbd map lstest/lsrbd1.img
```

#### 7. 查看 rbd 中已经存储的对象

```sh
# 以 rbd_data.13c53b180418 开头, 每一个都是4M

rados -p lstest ls | grep rbd_data.13c53b180418
```

#### 8. 查看指定的 object 落在哪个 pg 上和哪个 osd 上

```sh
ceph osd map lstest rbd_data.13c53b180418.000000000000043
```

#### 9. rbd 文件扩容大小

```sh
# 将 lsrbd1.img 块从之前的10G 改成现在 15G
rbd resize lstest/lsrbd1.img --size 15G

# 如果块设备已经挂在到机器上. 则机器上显示不会自动扩容。 需要执行:
resize2fs /dev/rbd0
```

### 二. rbd 回收站

#### 1. 将 rbd 文件移动到回收站, 并设置过期时间

```sh
rbd trash move lstest/lsrbd2.img --expires-at 20231017
rbd trash move pool002/testcrush.img --expires-at 20231017
```

#### 2. 查看回收站

```sh
[lsne@myk8smaster04v cephcluster]$ rbd trash -p lstest ls
13ce2b7c4952 lsrbd2.img
[lsne@myk8smaster04v cephcluster]$
[lsne@myk8smaster04v cephcluster]$ rbd trash -p pool002 ls
d369b805327e testcrush.img
```

#### 3. 将 lsrbd2.img 文件从回收站恢复

> 从哪个池子里删除的, 就只能恢复到哪个池子里

```sh
rbd trash restore -p lstest 13ce2b7c4952
rbd trash restore -p pool002 d369b805327e
```

### 三. 快照

#### 1. 创建一个带快照功能的 rbd 文件

```sh
rbd create lstest/lsrbd3.img --image-feature layering --size 10G
```

#### 2. 给文件打快照

```sh
# rbd snap create <pool name>/<rbd name>@<快照名>

rbd snap create lstest/lsrbd3.img@lsrbd3_snap_20231016
```

#### 3. 查看快照

```sh
rbd snap ls lstest/lsrbd3.img
```

#### 4. 回滚快照

> 回滚之前需要先 umount 盘, 否则看不到回滚后的内容

```sh
rbd snap rollback lstest/lsrbd3.img@lsrbd3_snap_20231016
```

#### 5. 删除指定镜像的指定快照

```sh
rbd snap rm lstest/lsrbd3.img@lsrbd3_snap_20231016
```

#### 6. 删除指定镜像的所有快照

```sh
rbd snap purge lstest/lsrbd3.img
```

#### 7. 保护快照

> 对重要数据, 如操作系统镜像文件, 需要做快照, 并保护起来, 阻止快照被删除

```sh
rbd snap protect lstest/lsrbd3.img@lsrbd3_template
```

#### 8. 取消保护

> 取消快照保护需要没有基于快照的克隆文件. 否则: rbd: unprotecting snap failed: (16) Device or resource busy

```sh
rbd snap unprotect lstest/lsrbd3.img@lsrbd3_template
```

#### 9. 克隆快照

> 克隆可以克隆到其他 pool 里

```sh
rbd clone lstest/lsrbd3.img@lsrbd3_template lstest/lsrbd_c1.img
rbd clone lstest/lsrbd3.img@lsrbd3_template pool001/lsrbd_c1.img
```

#### 10. 查看克隆文件详情

```sh
rbd -p pool001 info lsrbd_c1.img
```

#### 11. 查看快照都有哪些克隆文件

```sh
rbd children lstest/lsrbd3.img@lsrbd3_template
```

#### 12. 解除依赖关系

```sh
rbd flatten pool001/lsrbd_c1.img
```

#### 13. 备份导出

> 导出的可以是文件, 也可以是文件的快照

```sh
# 导出自 image 创建以来到 到当前时间点的全部数据
rbd export lstest/lsrbd3.img imgs/lsrbd3.img

# 导出自 image 创建以来到快照 lsrbd3_template 的全部数据
rbd export lstest/lsrbd3.img@lsrbd3_template imgs/lsrbd3_template.img
```

#### 14. 恢复导入

```sh
rbd import imgs/lsrbd3_template.img pool001/lsrbd3_import.img
```

#### 15. 增量备份导出

```sh
# 导出自 image 创建以来到 到当前时间点的全部数据
rbd export-diff lstest/lsrbd3.img imgs/lsrbd3_template.img

# 导出自 image 创建以来到 快照 lsrbd3_template 的全部数据
rbd export-diff lstest/lsrbd3.img@lsrbd3_template imgs/lsrbd3_template.img@lsrbd3_template

# 导出自快照 lsrbd3_template 以来到 快照 v10 的增量数据
rbd export-diff lstest/lsrbd3.img@v10 --from-snap lsrbd3_template imgs/lsrbd3_template.img@lsrbd3_template_v10
```

#### 16. 增量备份导入

```sh
rbd import-diff imgs/lsrbd3_template.img@lsrbd3_template pool001/lsrbd3_import.img
rbd import-diff imgs/lsrbd3_template.img@lsrbd3_template_v10 pool001/lsrbd3_import.img
```

## radosgw-admin 命令

### 一. 安装radosgw

> 看安装文档

### 二. 常用命令

#### 1. 创建用户

> 创建完成后, 输出结果中会有 username, access_key, secret_key 等信息

```sh
radosgw-admin user create --uid myuser01 --display-name "my friest username Demo"
```

```
{
    "user_id": "myuser01",
    "display_name": "my friest username Demo",
    "email": "",
    "suspended": 0,
    "max_buckets": 1000,
    "subusers": [],
    "keys": [
        {
            "user": "myuser01",
            "access_key": "REKB228Q7C9OIA8EKZ3X",
            "secret_key": "980HENBQkPxOlaoVMpdv6TojEZPkbSUfLC2VAi6g"
        }
    ],
    "swift_keys": [],
    "caps": [],
    "op_mask": "read, write, delete",
    "default_placement": "",
    "default_storage_class": "",
    "placement_tags": [],
    "bucket_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "user_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "temp_url_keys": [],
    "type": "rgw",
    "mfa_ids": []
}
```

#### 2. 查看用户列表

```sh
radosgw-admin user list
```

#### 3. 查看用户详细信息

```sh
radosgw-admin user info --uid myuser01
```

#### 4. 修改用户最大 bucket 数量

```sh
radosgw-admin user modify --uid=loki --max-buckets=4
```

#### 5. 对象增删查

> 详见 s3cmd 工具使用文档


## mds - cephfs 命令

### 一. 修改用户权限，使其只有 /mydata 目录的读写权限。 根目录只有只读权限

```sh
ceph auth caps client.myuser001 mon 'allow r' osd 'allow rw pool=mypool' mds 'allow r,allow rw path=/mydata'
```

### 挂载

```sh
mount -t ceph 10.51.97.41:6789,10.51.97.42:6789,10.51.97.43:6789:/mydata /test2 -o name=mydata,secret=AQBKqJtlfykUCBAAOQyZKn55ONi4YLmVZLBO4g==

#或
mount -t fuse.ceph 10.51.97.41:6789,10.51.97.42:6789,10.51.97.43:6789:/ /test -o ceph.id=admin,ceph.conf=/etc/ceph/ceph.conf
```

### 设置数据目录指定数据池

```sh
# 8 为数据池编号
setfattr -n ceph.dir.layout.pool -v 8 /test2/mydata/
```

### 查看属性

```sh
getfattr -n ceph.dir.layout.pool_name /test2/output/
getfattr -n ceph.dir.layout /test2/output/
getfattr -n ceph.file.layout /test2/test_xxh/a
```

### 删除数据池

```sh
ceph fs rm_data_pool cephfs mydata
ceph osd pool rm mydata mydata --yes-i-really-really-mean-it
```

### 查看指定的 fs 挂在目录下的文件数量等相关信息

```sh
getfattr -d -m ceph.dir.* /test/mydata/lsne/main/testbed
```

## 其他

### 1. 批量操作 mgr prometheus 配置参数

```sh
for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do echo "ceph config rm mgr.$i mgr/prometheus/scrape_interval; ceph config rm mgr.$i mgr/prometheus/stale_cache_strategy"; done;

for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do ceph config rm mgr.$i mgr/prometheus/scrape_interval; ceph config rm mgr.$i mgr/prometheus/stale_cache_strategy; done;


ceph config set mgr mgr/prometheus/stale_cache_strategy return

ceph config set mgr mgr/prometheus/scrape_interval 60


for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do ceph config get mgr.$i mgr/prometheus/scrape_interval; ceph config get mgr.$i mgr/prometheus/stale_cache_strategy; done;

ceph mgr module disable prometheus

ceph mgr module enable prometheus
```

#### 2. prometheus 开启 rbd 统计信息

```sh
for i in `ceph node ls | jq -r '.mgr | .[][0]'`;do ceph config get mgr.$i mgr/prometheus/rbd_stats_pools; done;

ceph config set mgr mgr/prometheus/rbd_stats_pools cinder.volumes.hdd,cinder.volumes.ssd,cinder.volumes.ssd-02,cinder.volumes.hdd-02

ceph config set mgr mgr/prometheus/rbd_stats_pools_refresh_interval 300

ceph mgr module disable prometheus

ceph mgr module enable prometheus
```

