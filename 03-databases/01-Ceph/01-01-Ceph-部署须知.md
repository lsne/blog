## ceph 部署关键点总结

#### 磁盘与缓存db盘

> ceph 写数据先写 journal 日志(这好像是老长存储格式使用的方式。新的没有 journal 了)

> 使用 SSD 作为 OSD 的日志能够很好地处理工作负载的峰值。

> 每个 SSD 磁盘最多给 4 或者 5 个 OSD 做日志

```
一旦超过这个数目的 OSD 的日志盘在同一个 SSD 磁盘上，这将是集群的性能瓶颈
同样，如果采用 XFS 或者 ext4 文件系统的多个 OSD 日志在同一个磁在上，
一旦这个盘出现错误.你将失去你的 OSD 及其数据。


这就是 Btrfs 的优势:
如果发生日志错误的 OSD 盘使用的是 Btrfs-based 文件系统，
它 将能够回滚到过去，这样只会导致最小的数据丢失或没有数据丢失。 
Btrfs 是一个写时复制文件系统，也就是说，
如果一个块的内容发生了变化，而针对这个块的写是独立进行的，因此能够保留旧的块。
对于这样一个场景下的损坏，数据依然可用，因为旧的内容依然可用。
```

> 推荐每个物理磁盘不少于 2GB 内存

> 建议先将新添加的 OSD 的权重设为 0, 之后逐步增加权重以减少 osd 再平衡产生的影响

#### PG 数量

```
一个 bucket 限制 2亿对象
一个 Pool 限制 64 PB 大小
一个 Pool 限制 65536 PG
```

> 1. 建议每个 OSD 上放置 50 - 100 个 PG, 最大不超过 250 个

> PG 数量一定要是 2 的 N 次幕
> 计算结果向后增加到  2 的 N 次幕

```
PG 总数= (OSD 总数 x 100) /最大副本数

结果必须舍入到最接近 2 的 N次幕的值。 比如:如果 Ceph 集群有 160 个 OSD 且副本数是 3 ，这样根据公式计算得到的 PG 总数是 5333.3 ，因此舍入这个值到最接近的 2 的 N 次幕的结果就是 8192 个 PG。
```

#### 每个 POOL 的 PG 总数

> PG 数量一定要是 2 的 N 次幕
> 计算结果向后增加到  2 的 N 次幕
> 每个池子的 PG 数量
> 每个池子的数量不一定绝对平均, 大池子多一些, 小池子少一些

```
PG 总数= ((OSD 总数 x 100) /最大副本数) /池数

# 不过要根据 pool 的实际存储数据量来规划 pg 数量。只要保证每一个pool的pg数量是 2 的 N次幕就行
```

#### 组件配置

```
mon: 对性能要求不高, 非生产可用虚拟机, 生产使用便宜的物理机
mgr: 和 mon 放在一台机器上
osd: 每个 osd 至少 2G 内存。  osd 机器建议两块网卡(public 和 cluster)
mds: 对内存要求高,  至少4C8G, 根据业务量增加, 专用物理机
```

#### Linux 对 ceph 的支持

> Linux 内核从 2.6.32 版本开始支持 Ceph, 从 2.6.34 及以上版本, 本地化地支持 Ceph

>  1. 如果你在 Ceph 对象存储上有大量的工作负载，则你应该使用专用的物理服务器来配置 RADOS 网关，另外你可以考虑将所有的 monitor 节点配置成 RADOS 网关

#### 1. 升级组件顺序

```
1) Monitor
2) OSD
3) MDS
4) RADOS 网关
```

## 参数

```
mon osd down out interval = 300  # osd 故障后多长时间标记为 down, 默认 300 秒
```

